<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Blog Atom Feed"><title data-react-helmet="true">Audio-Unit-Hosting-Guide-for-iOS | My Site</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Audio-Unit-Hosting-Guide-for-iOS | My Site"><meta data-react-helmet="true" name="description" content="Audio Unit Hosting Guide for iOS"><meta data-react-helmet="true" property="og:description" content="Audio Unit Hosting Guide for iOS"><meta data-react-helmet="true" property="og:url" content="http://localhost:9999//docs/音视频/学习资料/教程/Audio-Unit-Hosting-Guide-for-iOS"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="http://localhost:9999//docs/音视频/学习资料/教程/Audio-Unit-Hosting-Guide-for-iOS"><link rel="stylesheet" href="/styles.8cfbcdec.css">
<link rel="preload" href="/styles.9f2bd08d.js" as="script">
<link rel="preload" href="/runtime~main.b9916a1e.js" as="script">
<link rel="preload" href="/main.aad198f0.js" as="script">
<link rel="preload" href="/1.c6142e7c.js" as="script">
<link rel="preload" href="/259.bd219e04.js" as="script">
<link rel="preload" href="/260.8c75c7b8.js" as="script">
<link rel="preload" href="/935f2afb.b83995d3.js" as="script">
<link rel="preload" href="/258.77a3145c.js" as="script">
<link rel="preload" href="/f30035aa.b5f636a0.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">My Site</strong></a><a class="navbar__item navbar__link" href="/docs/">基础</a><a class="navbar__item navbar__link" href="/docs/编程语言/编程语言">语言</a><a class="navbar__item navbar__link" href="/docs/工具/常用工具">工具</a><a class="navbar__item navbar__link" href="/docs/应用开发/应用开发">应用开发</a><a class="navbar__item navbar__link" href="/docs/Apple/iOS开发/iOS开发">Apple</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/音视频/音视频">音视频</a><a class="navbar__item navbar__link" href="/docs/项目/项目">项目</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2N3Q"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_3NWk">🌜</span></div><div class="react-toggle-track-x"><span class="toggle_3NWk">🌞</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Search" aria-label="Search" class="navbar__search-input search-bar"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">My Site</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/">基础</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/编程语言/编程语言">语言</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/工具/常用工具">工具</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/应用开发/应用开发">应用开发</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/Apple/iOS开发/iOS开发">Apple</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs/音视频/音视频">音视频</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/项目/项目">项目</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_vMrn"><main class="docMainContainer_2iGs"><div class="container padding-vert--lg docItemWrapper_1bxp"><div class="row"><div class="col docItemCol_U38p"><div class="docItemContainer_a7m4"><article><header><h1 class="docTitle_Oumm">Audio-Unit-Hosting-Guide-for-iOS</h1></header><div class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-unit-hosting-guide-for-ios"></a>Audio Unit Hosting Guide for iOS<a class="hash-link" href="#audio-unit-hosting-guide-for-ios" title="Direct link to heading">#</a></h1><p>[toc]</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-unit-hosting-fundamentals"></a>Audio Unit Hosting Fundamentals<a class="hash-link" href="#audio-unit-hosting-fundamentals" title="Direct link to heading">#</a></h2><p>Direct use of audio units in your project is the correct choice only when you need the very highest degree of control, performance, or flexibility, or when you need a specific feature (such as acoustic echo cancelation) available only by using an audio unit directly.</p><p>仅当您需要最高程度的控制，性能或灵活性，或者需要直接使用音频单元才能使用的特定功能（例如回声消除）时，才可以在项目中直接使用音频单元是正确的选择 。</p><p>For an overview of iOS audio APIs, and guidance on when to use each one, refer to <a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/MultimediaPG/Introduction/Introduction.html#//apple_ref/doc/uid/TP40009767" target="_blank" rel="noopener noreferrer"><em>Multimedia Programming Guide</em></a>.</p><p>有关iOS音频API的概述以及何时使用每种API的指南，请参阅《多媒体编程指南》。</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-units-provide-fast-modular-audio-processing"></a>Audio Units Provide Fast, Modular Audio Processing<a class="hash-link" href="#audio-units-provide-fast-modular-audio-processing" title="Direct link to heading">#</a></h3><p>音频单元提供快速的模块化音频处理</p><p>modules</p><p>Use audio units directly, rather than by way of higher-level APIs, when you require one of the following:</p><ul><li>Simultaneous audio I/O (input and output) with low latency, such as for a VoIP (Voice over Internet Protocol) application</li></ul><p>低延迟的同时音频I / O（输入和输出），例如VoIP（互联网协议语音）应用</p><ul><li>Responsive playback of synthesized sounds, such as for musical games or synthesized musical instruments</li></ul><p>响应性播放合成声音，例如音乐游戏或合成乐器 (synthesized: 合成 ， synchronization：同步)</p><ul><li>Use of a specific audio unit feature such as acoustic echo cancelation, mixing, or tonal equalization</li></ul><p>使用特定的音频单元功能，例如回声消除，混音或音调均衡</p><ul><li>A processing-chain architecture that lets you assemble audio processing modules into flexible networks. This is the only audio API in iOS offering this capability.</li></ul><p>一种处理链体系结构，可让您将音频处理模块组装到灵活的网络中。 这是iOS中唯一提供此功能的音频API。</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-units-in-ios"></a>Audio Units in iOS<a class="hash-link" href="#audio-units-in-ios" title="Direct link to heading">#</a></h4><p>iOS provides seven audio units arranged into four categories by purpose, as shown in Table 1-1.</p><p>iOS提供了七个音频单元，按目的将音频单元分为四个类别，如表1-1所示。（arranged： 安排，整理）</p><table><thead><tr><th><strong>Purpose</strong></th><th><strong>Audio units</strong></th></tr></thead><tbody><tr><td><em>Effect</em></td><td>iPod Equalizer</td></tr><tr><td><em>Mixing</em></td><td>3D Mixer Multichannel Mixer</td></tr><tr><td><em>I/O</em></td><td>Remote I/OVoice-Processing I/OGeneric Output</td></tr><tr><td><em>Format conversion</em></td><td>Format Converter</td></tr></tbody></table><p>iPod  Equalizer：iPod 均衡器</p><p>note: The iOS dynamic plug-in architecture does not support third-party audio units. That is, the only audio units available for dynamic loading are those provided by the operating system.</p><p>iOS动态插件体系结构不支持第三方音频单元。 即，唯一可用于动态加载的音频单元是操作系统提供的那些音频单元。 (dynamic plug-in architecture)</p><ul><li>Effect Unit</li></ul><p>iOS 4 provides one effect unit, the <em>iPod Equalizer</em>, the same equalizer used by the built-in iPod app. To view the iPod app’s user interface for this audio unit, go to Settings &gt; iPod &gt; EQ. When using this audio unit, you must provide your own UI. This audio unit offers a set of preset equalization curves such as Bass Booster, Pop, and Spoken Word.</p><p>该音频单元提供了一组预设的均衡曲线，例如低音助推器，流行音乐和口语单词。</p><ul><li>Mixer Units</li></ul><p>iOS provides two mixer units. The <em>3D Mixer unit</em> is the foundation upon which OpenAL is built. In most cases, if you need the features of the 3D Mixer unit, your best option is to use OpenAL which provides a higher level API that is well suited for game apps. For sample code that shows how to use OpenAL, see the sample code project <em>oalTouch</em>.</p><p>iOS提供了两个混音器单元。 3D混合器单元是构建OpenAL的基础。 在大多数情况下，如果您需要3D混合器单元的功能，最好的选择是使用OpenAL，它提供了更适合游戏应用程序的更高级别的API。 有关显示如何使用OpenAL的示例代码，请参见示例代码项目oalTouch。</p><p>is the foundation upon which OpenAL is built.</p><p>is well suited for game apps</p><p>The <em>Multichannel Mixer unit</em> provides mixing for any number of mono or stereo streams, with a stereo output. You can turn each input on or off, set its input gain, and set its stereo panning position. For a demonstration of how to use this audio unit, see the sample code project <em>Audio Mixer (MixerHost)</em>.</p><blockquote><p>多声道混音器单元可为任意数量的单声道或立体声流提供混音，并带有立体声输出。 您可以打开或关闭每个输入，设置其输入增益，并设置其立体声声像位置。 有关如何使用此音频单元的演示，请参见示例代码项目Audio Mixer（MixerHost）。</p><p>mono: 单声道</p><p>stereo：立体声</p><p>gain：增益</p></blockquote><ul><li>I/O Units</li></ul><p>iOS provides three I/O units. The <em>Remote I/O unit</em> is the most commonly used. It connects to input and output audio hardware and gives you low–latency access to individual incoming and outgoing audio sample values. It provides format conversion between the hardware audio formats and your application audio format, doing so by way of an included Format Converter unit. For sample code that shows how to use the Remote I/O unit, see the sample code project <a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770" target="_blank" rel="noopener noreferrer"><em>aurioTouch</em></a>.</p><p>The <em>Voice-Processing I/O unit</em> extends the Remote I/O unit by adding acoustic echo cancelation for use in a VoIP or voice-chat application. It also provides automatic gain correction, adjustment of voice-processing quality, and muting.</p><p>The <em>Generic Output unit</em> does not connect to audio hardware but rather provides a mechanism for sending the output of a processing chain to your application. You would typically use the Generic Output unit for offline audio processing.</p><blockquote><p>individual incoming and outgoing audio sample values. 各个传入和传出的音频样本值。</p><p>an included Format Converter unit. 附带的格式转换单元</p><p>acoustic echo cancellation：声学的回声消除</p><p>automatic gain correction: 自动修正增益</p><p>adjustment of voice-processing quality, muting: 语音处理质量调整和静音</p><p>offline audio processing： 脱机音频处理</p><p>a mechanism for sending the output of a processing chaning ti you application : 一种将处理链的输出发送到您的应用程序的机制</p></blockquote><ul><li>Format Converter Unit</li></ul><p>iOS 4 provides one <em>Format Converter unit</em>, which is typically used indirectly by way of an I/O unit.</p><p>iOS 4提供了一个格式转换器单元，该单元通常通过I / O单元间接使用。</p><p>typically: 通常</p><p>by way of： 通过经由。</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="use-the-two-audio-unit-apis-in-concert"></a>Use the Two Audio Unit APIs in Concert<a class="hash-link" href="#use-the-two-audio-unit-apis-in-concert" title="Direct link to heading">#</a></h4><p>iOS has one API for working with audio units directly and another for manipulating audio processing graphs. When you host audio units in your app, you use both APIs in concert.</p><blockquote><p>iOS有一个API可直接使用音频单元，而另一个API可用于处理音频处理图。 在应用中托管音频单元时，您会同时使用两个API。</p><p>manipulating：操纵</p><p>Concert：音乐会；合奏</p></blockquote><ul><li>To work with audio units directly—configuring and controlling them—use the functions described in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_component_services" target="_blank" rel="noopener noreferrer">Audio Unit Component Services Reference</a></em>.</li><li>To create and configure an audio processing graph (a processing chain of audio units) use the functions described in <em><a href="https://developer.apple.com/documentation/audiotoolbox/audio_unit_processing_graph_services" target="_blank" rel="noopener noreferrer">Audio Unit Processing Graph Services Reference</a></em></li></ul><p>There is some overlap between the two APIs and you are free to mix and match according to your programming style. The audio unit API and audio processing graph API each provide functions for:</p><ul><li>Obtaining references to the dynamically-linkable libraries that define audio units</li><li>Instantiating audio units</li><li>Interconnecting audio units and attaching render callback functions</li><li>Starting and stopping audio flow</li></ul><blockquote><p>overlap：重叠</p><p>you are free to mix and match：你可以自由的混合和匹配</p><p>Instantiating audio units：实例化音频单元 （instantiate）</p><p>Interconnecting audio units and attaching render callback functions：互连音频单元并附加渲染回调函数</p></blockquote><p>This document provides code examples for using both APIs but focuses on the audio processing graph API. Where there is a choice between the two APIs in your code, use the processing graph API unless you have a specific reason not to. Your code will be more compact, easier to read, and more amenable to supporting dynamic reconfiguration (see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW5" target="_blank" rel="noopener noreferrer">Audio Processing Graphs Provide Thread Safety</a>).</p><blockquote><p>Your code will be more compact, easier to read, and more amenable to supporting dynamic reconfiguration</p><p>您的代码将更紧凑，更易于阅读，并且更适合支持动态重新配置</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="use-identifiers-to-specify-and-obtain-audio-units"></a>Use Identifiers to Specify and Obtain Audio Units<a class="hash-link" href="#use-identifiers-to-specify-and-obtain-audio-units" title="Direct link to heading">#</a></h4><blockquote><p>使用标识符（Identifiers）来指定（Specify）和获取（Obtain）音频单元</p></blockquote><p>To find an audio unit at runtime, start by specifying its type, subtype, and manufacturer keys in an audio component description data structure. You do this whether using the audio unit or audio processing graph API. Listing 1-1 shows how.</p><blockquote><p>To find an audio unit at runtime, start by specifying its type, subtype, and manufacturer keys in an audio component description data structure.</p><p>要在运行时查找音频单元，请先在音频组件描述数据结构中指定其类型，子类型和制造商密钥。</p></blockquote><p>Listing 1-1  Creating an audio component description to identify an audio unit</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioComponentDescription ioUnitDescription;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">ioUnitDescription.componentType          = kAudioUnitType_Output;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">ioUnitDescription.componentSubType       = kAudioUnitSubType_RemoteIO;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">ioUnitDescription.componentManufacturer  = kAudioUnitManufacturer_Apple;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">ioUnitDescription.componentFlags         = 0;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">ioUnitDescription.componentFlagsMask     = 0;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>This description specifies exactly one audio unit—the Remote I/O unit. The keys for this and other iOS audio units are listed in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW14" target="_blank" rel="noopener noreferrer">Identifier Keys for Audio Units</a>. Note that all iOS audio units use the <code>kAudioUnitManufacturer_Apple</code> key for the <code>componentManufacturer</code> field.</p><p>To create a wildcard description, set one or more of the type/subtype fields to 0. For example, to match all the I/O units, change Listing 1-1 to use a value of 0 for the <code>componentSubType</code> field.</p><p>With a description in hand, you obtain a reference to the library for the specified audio unit (or set of audio units) using either of two APIs. The audio unit API is shown in Listing 1-2.</p><blockquote><p>With a description in hand, you obtain a reference to the library for the specified audio unit (or set of audio units) using either of two APIs.</p><p>有了描述，您可以使用两个API之一获取对指定音频单元（或一组音频单元）的库的引用。</p></blockquote><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">Listing 1-2  Obtaining an audio unit instance using the audio unit API</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioComponent foundIoUnitReference = AudioComponentFindNext (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                          NULL,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                          &amp;ioUnitDescription</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                      );</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnit ioUnitInstance;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioComponentInstanceNew (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    foundIoUnitReference,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;ioUnitInstance</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Passing <code>NULL</code> to the first parameter of <code>AudioComponentFindNext</code> tells this function to find the first system audio unit matching the description, using a system-defined ordering. If you instead pass a previously found audio unit reference in this parameter, the function locates the next audio unit matching the description. This usage lets you, for example, obtain references to all of the I/O units by repeatedly calling <code>AudioComponentFindNext</code>.</p><p>The second parameter to the <code>AudioComponentFindNext</code> call refers to the audio unit description defined in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW4" target="_blank" rel="noopener noreferrer">Listing 1-1</a>.</p><p>The result of the <code>AudioComponentFindNext</code> function is a reference to the dynamically-linkable library that defines the audio unit. Pass the reference to the <code>AudioComponentInstanceNew</code> function to instantiate the audio unit, as shown in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW9" target="_blank" rel="noopener noreferrer">Listing 1-2</a>.</p><p>You can instead use the audio processing graph API to instantiate an audio unit. Listing 1-3 shows how.</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">Listing 1-3  Obtaining an audio unit instance using the audio processing graph API</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Declare and instantiate an audio processing graph</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraph processingGraph;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">NewAUGraph (&amp;processingGraph);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Add an audio unit node to the graph, then instantiate the audio unit</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUNode ioNode;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphAddNode (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    processingGraph,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;ioUnitDescription,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;ioNode</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphOpen (processingGraph); // indirectly performs audio unit instantiation</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Obtain a reference to the newly-instantiated I/O unit</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnit ioUnit;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphNodeInfo (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    processingGraph,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    ioNode,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NULL,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;ioUnit</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><blockquote><p>indirectly performs audio unit instantiation</p><p>间接执行音频单元实例化</p></blockquote><p>This code listing introduces <code>AUNode</code>, an opaque type that represents an audio unit in the context of an audio processing graph. You receive a reference to the new audio unit instance, in the <em>ioUnit</em> parameter, on output of the <code>AUGraphNodeInfo</code> function call.</p><p>The second parameter to the <code>AUGraphAddNode</code> call refers to the audio unit description defined in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW4" target="_blank" rel="noopener noreferrer">Listing 1-1</a>.</p><p>Having obtained an audio unit instance, you can configure it. To do so, you need to learn about two audio unit characteristics, <em>scopes</em> and <em>elements</em>.</p><blockquote><p>introduces : 介绍；引用</p><p>an opaque type ：一种不透明的类型</p><p>To do so, you need to learn about two audio unit characteristics, <em>scopes</em> and <em>elements</em>.</p><p>为此，您需要了解两个音频单元特性，范围和元素。</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="use-scopes-and-elements-to-specify-parts-of-audio-units"></a>Use Scopes and Elements to Specify Parts of Audio Units<a class="hash-link" href="#use-scopes-and-elements-to-specify-parts-of-audio-units" title="Direct link to heading">#</a></h4><p>The parts of an audio unit are organized into scopes and elements, as shown in Figure 1-2. When invoking a function to configure or control an audio unit, you specify the scope and element to identify the specific target of the function.</p><blockquote><p>are organized into scopes and elements: 安装作用域和元素进行组织</p><p>you specify the scope and element to identify the specific target of the function: 你需要指定作用域和元素以标识功能的特定目标</p></blockquote><p><img alt="img" src="/assets/images/audioUnitScopes_2x-11043ca22732e793c12d0e89ee4b3276.png"></p><p>A <em>scope</em> is a programmatic context within an audio unit. Although the name <em>global scope</em> might suggest otherwise, these contexts are never nested. You specify the scope you are targeting by using a constant from the <code>Audio Unit Scopes</code> enumeration.</p><blockquote><p>Although the name <em>global scope</em> might suggest otherwise： 尽管“<em>global scope</em> ”可能暗示其他含义</p><p>these contexts are never nested： 这些上下文环境从不会嵌套</p></blockquote><p>An <em>element</em> is a programmatic context nested within an audio unit scope. When an element is part of an input or output scope, it is analogous to a signal bus in a physical audio device—and for that reason is sometimes called a bus. These two terms—<em>element</em> and <em>bus</em>—refer to exactly the same thing in audio unit programming. This document uses “bus” when emphasizing signal flow and uses “element” when emphasizing a specific functional aspect of an audio unit, such the input and output elements of an I/O unit (see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW43" target="_blank" rel="noopener noreferrer">Essential Characteristics of I/O Units</a>).</p><blockquote><p>it is analogous to a signal bus in a physical audio device： 他类似于物理音频设备中的信号总线</p><p>These two terms—<em>element</em> and <em>bus</em>—refer to exactly the same thing in audio unit programming. ： 这两个术语在audio unit 编程里面指的是完全相同的事物</p><p>when emphasizing signal flow ：当强调信号流定的时候</p><p>when emphasizing a specific functional aspect of an audio unit：当强调audio unit 的特定功能方便。</p></blockquote><p>You specify an element (or bus) by its zero-indexed integer value. If setting a property or parameter that applies to a scope as a whole, specify an element value of 0.</p><blockquote><p>applies to a scope as a whole： 应用于整个范围</p><p>specify an element value of 0.： 指定element的值为0</p></blockquote><p>Figure 1-2 illustrates one common architecture for an audio unit, in which the numbers of elements on input and output are the same. However, various audio units use various architectures. A mixer unit, for example, might have several input elements but a single output element. You can extend what you learn here about scopes and elements to any audio unit, despite these variations in architecture.</p><blockquote><p>illustrates：阐述了</p><p>despite these variations in architecture. 尽管架构上有变化</p></blockquote><p>The global scope, shown at the bottom of Figure 1-2, applies to the audio unit as a whole and is not associated with any particular audio stream. It has exactly one element, namely element 0. Some properties, such as maximum frames per slice (kAudioUnitProperty_MaximumFramesPerSlice), apply only to the global scope.</p><blockquote><p>applies to the audio unit as a whole： 全局范围适用于整个音频单元</p><p>is not associated with any particular audio stream：不和任何特定的音频流相关联</p><p>apply only to the global scope.：仅仅适用于全局作用域</p><p>action scope：<strong>作用域</strong></p></blockquote><p>The input and output scopes participate directly in moving one or more audio streams through the audio unit. As you’d expect, audio enters at the input scope and leaves at the output scope. A property or parameter may apply to an input or output scope as a whole, as is the case for the element count property (<code>kAudioUnitProperty_ElementCount</code>), for example. Other properties and parameters, such as the enable I/O property (<code>kAudioOutputUnitProperty_EnableIO</code>) or the volume parameter (<code>kMultiChannelMixerParam_Volume</code>), apply to a specific element within a scope.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="use-properties-to-configure-audio-units"></a>Use Properties to Configure Audio Units<a class="hash-link" href="#use-properties-to-configure-audio-units" title="Direct link to heading">#</a></h4><p>An audio unit property is a key-value pair you can use to configure an audio unit. The key for a property is a unique integer with an associated mnemonic identifier, such as kAudioUnitProperty_MaximumFramesPerSlice = 14. Apple reserves property keys from 0 through 63999. In Mac OS X, third-party audio units make use of keys above this range.</p><blockquote><p>an associated mnemonic identifier,： 一个关联的助记标识符</p><p>keys above this range.：此范围的键值</p></blockquote><p>The value for each property is of a designated data type and has a designated read/write access, as described in Audio Unit Properties Reference. To set any property on any audio unit, you use one flexible function: AudioUnitSetProperty. Listing 1-4 shows a typical use of this function, with comments highlighting how to specify the scope and element as well as indicating the key and value for the property.</p><blockquote><p>a designated data type： 指定的数据类型</p><p>a designated read/write access：指定的读写访问访问权限</p><p>as well as indicating the key and value for the property.：以及指定这个属性的键值</p><p>with comments highlighting ：注释着重说明了</p></blockquote><p>listing 1-4  Using scope and element when setting a property</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">UInt32 busCount = 2;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">OSStatus result = AudioUnitSetProperty (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    mixerUnit,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitProperty_ElementCount,   // the property key</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitScope_Input,             // the scope to set the property on</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    0,                                 // the element to set the property on</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;busCount,                         // the property value</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    sizeof (busCount)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Here are a few properties you’ll use frequently in audio unit development. Become familiar with each of these by reading its reference documentation and by exploring Apple’s audio unit sample code projects such as <em>Audio Mixer (MixerHost)</em>:</p><ul><li><code>kAudioOutputUnitProperty_EnableIO</code>, for enabling or disabling input or output on an I/O unit. By default, output is enabled but input is disabled.</li><li><code>kAudioUnitProperty_ElementCount</code>, for configuring the number of input elements on a mixer unit, for example.</li><li><code>kAudioUnitProperty_MaximumFramesPerSlice</code>, for specifying the maximum number of frames of audio data an audio unit should be prepared to produce in response to a render call. For most audio units, in most scenarios, you must set this property as described in the reference documentation. If you don’t, your audio will stop when the screen locks.</li><li><code>kAudioUnitProperty_StreamFormat</code>, for specifying the audio stream data format for a particular audio unit input or output bus.</li></ul><p>Most property values can be set only when an audio unit is uninitialized. Such properties are not intended to be changed by the user. Some, though, such as the <code>kAudioUnitProperty_PresentPreset</code> property of the iPod EQ unit, and the <code>kAUVoiceIOProperty_MuteOutput</code> property of the Voice-Processing I/O unit, <em>are</em> intended to be changed while playing audio.</p><p>To discover a property’s availability, access its value, and monitor changes to its value, use the following functions:</p><ul><li><code>AudioUnitGetPropertyInfo</code>—To discover whether a property is available; if it is, you are given the data size for its value and whether or not you can change the value</li><li><code>AudioUnitGetProperty</code>, <code>AudioUnitSetProperty</code>—To get or set the value of a property</li><li><code>AudioUnitAddPropertyListener</code>, <code>AudioUnitRemovePropertyListenerWithUserData</code>—To install or remove a callback function for monitoring changes to a property’s value</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="use-parameters-and-uikit-to-give-users-control"></a>Use Parameters and UIKit to Give Users Control<a class="hash-link" href="#use-parameters-and-uikit-to-give-users-control" title="Direct link to heading">#</a></h4><p>An <em>audio unit parameter</em> is a user-adjustable setting that can change while an audio unit is producing audio. Indeed, the intention of most parameters (such as volume or stereo panning position) is real-time adjustment of the processing that an audio unit is performing.</p><blockquote><p>user-adjustable setting: 用户可调整的设置</p><p>Indeed, the intention of most parameters ：事实上，大多数参数的意图</p><p>stereo panning position：立体声声像位置/立体声平移</p></blockquote><p>Like an audio unit property, an audio unit parameter is a key-value pair. The key is defined by the audio unit it applies to. It is always an enumeration constant, such as <code>kMultiChannelMixerParam_Pan = 2</code>, that is unique to the audio unit but not globally unique.</p><p>Unlike property values, every parameter value is of the same type: 32-bit floating point. The permissible range for a value, and the unit of measure that it represents, are determined by the audio unit’s implementation of the parameter. These and other aspects of the parameters in iOS audio units are described in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_parameters" target="_blank" rel="noopener noreferrer">Audio Unit Parameters Reference</a></em>.</p><blockquote><p>32-bit floating point :32位浮点型</p><p>The permissible range for a value：值的允许范围</p><p>the unit of measure that it represents： 其表示的度量单位</p><p>determined by the audio unit’s implementation of the parameter： 决定于audio unit对于这些参数的实现</p></blockquote><p>To get or set a parameter value, use one of the following functions, which are fully described in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_component_services" target="_blank" rel="noopener noreferrer">Audio Unit Component Services Reference</a></em>:</p><ul><li><code>AudioUnitGetParameter</code></li><li><code>AudioUnitSetParameter</code></li></ul><p>To allow users to control an audio unit, give them access to its parameters by way of a user interface. Start by choosing an appropriate class from UIKit framework to represent the parameter. For example, for an on/off feature, such as the Multichannel Mixer unit’s kMultiChannelMixerParam_Enable parameter, you could use a UISwitch object. For a continuously varying feature, such as stereo panning position as provided by the kMultiChannelMixerParam_Pan parameter, you could use a UISlider object.</p><blockquote><p>an appropriate class：一个合适的类</p><p>For a continuously varying feature: 对于一个持续变化的功能</p></blockquote><p>Convey the value of the UIKit object’s current configuration—such as the position of the slider thumb for a <code>UISlider</code>—to the audio unit. Do so by wrapping the <code>AudioUnitSetParameter</code> function in an <code>IBAction</code> method and establishing the required connection in Interface Builder. For sample code illustrating how to do this, see the sample code project <em>Audio Mixer (MixerHost)</em>.</p><blockquote><p>the slider thumb for a <code>UISlider</code>： <code>UISlider</code>的滑块</p><p>wrap：封装，包装</p><p>establishing the required connection in Interface Builder：建立需要的连接</p><p>illustrating how to do this：展示怎么去用这个</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="essential-characteristics-of-io-units"></a>Essential Characteristics of I/O Units<a class="hash-link" href="#essential-characteristics-of-io-units" title="Direct link to heading">#</a></h4><blockquote><p>I / O单元的基本特征</p></blockquote><p>I/O units are the one type of audio unit used in every audio unit app and are unusual in several ways. For both these reasons, you must become acquainted with the essential characteristics of I/O units to gain facility in audio unit programming.</p><p>An I/O unit contains exactly two elements, as you see in Figure 1-3.</p><blockquote><p>become acquainted with：熟练的</p><p>gain facility in audio unit programming.：在audio unit 编程中获得便利</p></blockquote><p><strong>Figure 1-3</strong> The architecture of an I/O unit</p><p><img src="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/Art/IO_unit_2x.png" alt="img"></p><p>Although these two elements are parts of one audio unit, your app treats them largely as independent entities. For example, you employ the enable I/O property (<code>kAudioOutputUnitProperty_EnableIO</code>) to enable or disable each element independently, according to the needs of your app.</p><p>Element 1 of an I/O unit connects directly to the audio input hardware on a device, represented in the figure by a microphone. This hardware connection—at the input scope of element 1—is opaque to you. Your first access to audio data entering from the input hardware is at the output scope of element 1.</p><p>Similarly, element 0 of an I/O unit connects directly the audio output hardware on a device, represented in Figure 1-3 by the loudspeaker. You can convey audio to the input scope of element 0, but its output scope is opaque.</p><p>Working with audio units, you’ll often hear the two elements of an I/O unit described not by their numbers but by name:</p><ul><li>The <em>input element</em> is element 1 (mnemonic device: the letter “I” of the word “Input” has an appearance similar to the number 1)</li><li>The <em>output element</em> is element 0 (mnemonic device: the letter “O” of the word “Output” has an appearance similar to the number 0)</li></ul><p>As you see in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW35" target="_blank" rel="noopener noreferrer">Figure 1-3</a>, each element itself has an input scope and an output scope. For this reason, describing these parts of an I/O unit may get a bit confusing. For example, you would say that in a simultaneous I/O app, you receive audio from the output scope of the input element and send audio to the input scope of the output element. When you need to, return to this figure.</p><p>Finally, I/O units are the only audio units capable of starting and stopping the flow of audio in an audio processing graph. In this way, the I/O unit is in charge of the audio flow in your audio unit app.</p><blockquote><p>the I/O unit is in charge of the audio flow in your audio unit app： I/O单元负责你的audio unit APP的音频流 audio flow</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-processing-graphs-manage-audio-units"></a>Audio Processing Graphs Manage Audio Units<a class="hash-link" href="#audio-processing-graphs-manage-audio-units" title="Direct link to heading">#</a></h3><p>An <em>audio processing graph</em> is a Core Foundation–style opaque type, <code>AUGraph</code>, that you use to construct and manage an audio unit processing chain. A graph can leverage the capabilities of multiple audio units and multiple render callback functions, allowing you to create nearly any audio processing solution you can imagine.</p><blockquote><p>leverage the capabilities of multiple audio units and multiple render callback functions: 利用 “multiple audio units and multiple render callback functions”的能力</p></blockquote><p>The <code>AUGraph</code> type adds thread safety to the audio unit story: It enables you to reconfigure a processing chain on the fly. For example, you could safely insert an equalizer, or even swap in a different render callback function for a mixer input, while audio is playing. In fact, the <code>AUGraph</code> type provides the only API in iOS for performing this sort of dynamic reconfiguration in an audio app.</p><blockquote><p>on the fly： 动态的（飞行中）</p></blockquote><p>The audio processing graph API uses another opaque type, AUNode, to represent an individual audio unit within the context of a graph. When using a graph, you usually interact with nodes as proxies for their contained audio units rather than interacting with the audio units directly.</p><blockquote><p>as proxies for their contained audio units：作为他们包含的audio unit 的proxy，代理</p></blockquote><p>When putting a graph together, however, you must configure each audio unit, and to do that you must interact directly with the audio units by way of the audio unit API. Audio unit nodes, per se, are not configurable. In this way, constructing a graph requires you to use the both APIs, as explained in Use the Two Audio Unit APIs in Concert.</p><blockquote><p>Audio unit nodes per se, are not configurable.： Audio unit nodes （per se）本身是不可配置的</p></blockquote><p>You can also use an <code>AUNode</code> instance as an element in a complex graph by defining the node to represent a complete audio processing subgraph. In this case, the I/O unit at the end of the subgraph must be a Generic Output unit—the one type of I/O unit that does not connect to device hardware.</p><blockquote><p>an element in a complex graph： 复杂图的元素</p></blockquote><p>In broad strokes, constructing an audio processing graph entails three tasks:</p><p>Adding nodes to a graph
Directly configuring the audio units represented by the nodes
Interconnecting the nodes</p><blockquote><p>In broad strokes : 广义范围内，概括的说</p><p>entails three tasks: 需要3个任务</p></blockquote><p>For details on these tasks and on the rest of the audio processing graph life cycle, refer to <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW1" target="_blank" rel="noopener noreferrer">Constructing Audio Unit Apps</a>. For a complete description of this rich API, see <em><a href="https://developer.apple.com/documentation/audiotoolbox/audio_unit_processing_graph_services" target="_blank" rel="noopener noreferrer">Audio Unit Processing Graph Services Reference</a></em>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="an-audio-processing-graph-has-exactly-one-io-unit"></a>An Audio Processing Graph Has Exactly One I/O Unit<a class="hash-link" href="#an-audio-processing-graph-has-exactly-one-io-unit" title="Direct link to heading">#</a></h4><blockquote><p>音频处理图只有一个I / O单元</p></blockquote><p>Every audio processing graph has one I/O unit, whether you are doing recording, playback, or simultaneous I/O. The I/O unit can be any one of those available in iOS, depending on the needs of your app. For details on how I/O units fit into the architecture of an audio processing graph in various usage scenarios, see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW2" target="_blank" rel="noopener noreferrer">Start by Choosing a Design Pattern</a>.</p><p>Graphs let you start and stop the flow of audio by way of the <code>AUGraphStart</code> and <code>AUGraphStop</code> functions. These functions, in turn, convey the start or stop message to the I/O unit by invoking its <code>AudioOutputUnitStart</code> or <code>AudioOutputUnitStop</code> function. In this way, a graph’s I/O unit is in charge of the audio flow in the graph.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-processing-graphs-provide-thread-safety"></a>Audio Processing Graphs Provide Thread Safety<a class="hash-link" href="#audio-processing-graphs-provide-thread-safety" title="Direct link to heading">#</a></h4><p>The audio processing graph API employs a “to-do list” metaphor to provide thread safety. Certain functions in this API add a unit of work to a list of changes to execute later. After you specify a complete set of changes, you then ask the graph to implement them.</p><p>Here are some common reconfigurations supported by the audio processing graph API, along with their associated functions:</p><ul><li>Adding or removing audio unit nodes (<code>AUGraphAddNode</code>, <code>AUGraphRemoveNode</code>)</li><li>Adding or removing connections between nodes (<code>AUGraphConnectNodeInput</code>, <code>AUGraphDisconnectNodeInput</code>)</li><li>Connecting a render callback function to an input bus of an audio unit (<code>AUGraphSetNodeInputCallback</code>)</li></ul><p>Let’s look at an example of reconfiguring a running audio processing graph. Say, for example, you’ve built a graph that includes a Multichannel Mixer unit and a Remote I/O unit, for mixed playback of two synthesized sounds. You feed the sounds to two input buses of the mixer. The mixer output goes to the output element of the I/O unit and on to the output audio hardware. Figure 1-4 depicts this architecture.</p><p><strong>Figure 1-4</strong> A simple audio processing graph for playback<img alt="img" src="/assets/images/AudioProcessingGraphBeforeEQ_2x-8acd55494667e68d49e3df0ff79f303f.png"></p><p>Now, say the user wants to insert an equalizer into one of the two audio streams. To do that, add an iPod EQ unit between the feed of one of the sounds and the mixer input that it goes to, as shown in Figure 1-5.</p><p><strong>Figure 1-5</strong> The same graph after inserting an equalizer<img alt="img" src="/assets/images/AudioProcessingGraphWithEQ_2x-8176416355dc7732cb9e2c7a0d6fa96a.png"></p><p>The steps to accomplish this live reconfiguration are as follows:</p><ol><li>Disconnect the “beats sound” callback from input 1 of the mixer unit by calling <code>AUGraphDisconnectNodeInput</code>.</li><li>Add an audio unit node containing the iPod EQ unit to the graph. Do this by specifying the iPod EQ unit with an <code>AudioComponentDescription</code> structure, then calling <code>AUGraphAddNode</code>. At this point, the iPod EQ unit is instantiated but not initialized. It is owned by the graph but is not yet participating in the audio flow.</li><li>Configure and initialize the iPod EQ unit. In this example, this entails a few things:<ul><li>Call the <code>AudioUnitGetProperty</code> function to retrieve the stream format (<code>kAudioUnitProperty_StreamFormat</code>) from the mixer input.</li><li>Call the <code>AudioUnitSetProperty</code> function twice, once to set that stream format on the iPod EQ unit’s input and a second time to set it on the output. (For a complete description of how to configure an iPod EQ unit, see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW4" target="_blank" rel="noopener noreferrer">Using Effect Units</a>.)</li><li>Call the <code>AudioUnitInitialize</code> function to allocate resources for the iPod EQ unit and prepare it to process audio. This function call is not thread-safe, but you can (and must) perform it at this point in the sequence, when the iPod EQ unit is not yet participating actively in the audio processing graph because you have not yet called the <code>AUGraphUpdate</code> function.</li></ul></li><li>Attach the “beats sound” callback function to the input of the iPod EQ by calling <code>AUGraphSetNodeInputCallback</code>.</li></ol><p>In the preceding list, steps 1, 2, and 4—all of them <code>AUGraph*</code> function calls—were added to the graph’s “to-do” list. Call <code>AUGraphUpdate</code> to execute these pending tasks. On successful return of the <code>AUGraphUpdate</code> function, the graph has been dynamically reconfigured and the iPod EQ is in place and processing audio.</p><blockquote><p>In the preceding list: 在前面的列表</p><p>these pending tasks： 这些挂起的任务</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-flows-through-a-graph-using-pull"></a>Audio Flows Through a Graph Using “Pull”<a class="hash-link" href="#audio-flows-through-a-graph-using-pull" title="Direct link to heading">#</a></h4><p>In an audio processing graph, the consumer calls the provider when it needs more audio data. There is a flow of requests for audio data, and this flow proceeds in a direction opposite to that of the flow of audio. Figure 1-6 illustrates this mechanism.</p><blockquote><p>There is a flow of requests for audio data：有一个音频数据的请求流</p><p>illustrates this mechanism.: 说明了这个机制</p><p>this flow proceeds in a direction opposite to that of the flow of audio.：这个流朝着音频流相返的方向进行</p></blockquote><p><strong>Figure 1-6</strong> The pull mechanism of audio data flow<img alt="img" src="/assets/images/pull_model_2x-5f61abb52f3491dda9b31fc8d9cc5871.png"></p><p>Each request for a set of data is known as a <em>render call</em> or, informally, as a <em>pull</em>. The figure represents render calls as gray “control flow” arrows. The data requested by a render call is more properly known as a set of <em>audio sample frames</em> (see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Glossary/core_audio_glossary.html#//apple_ref/doc/uid/TP40004453-CH210-CHDHCAJI" target="_blank" rel="noopener noreferrer">frame</a>).</p><p>In turn, a set of audio sample frames provided in response to a render call is known as a <em>slice</em>. (See <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Glossary/core_audio_glossary.html#//apple_ref/doc/uid/TP40004453-CH210-SW165" target="_blank" rel="noopener noreferrer">slice</a>.) The code that provides the slice is known as a <em>render callback function</em>, described in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW27" target="_blank" rel="noopener noreferrer">Render Callback Functions Feed Audio to Audio Units</a>.</p><p>Here is how the pull proceeds in Figure 1-6:</p><blockquote><p>render call：渲染回调</p><p>more properly known as a set of <em>audio sample frames</em>： 更合适的被称为音频采样帧</p><p>In turn：依次</p><p>pull proceeds：拉动过程</p></blockquote><ol><li>After you call the <code>AUGraphStart</code> function, the virtual output device invokes the render callback of the Remote I/O unit’s output element. This invocation asks for one slice of processed audio data frames.</li><li>The render callback function of the Remote I/O unit looks in its input buffers for audio data to process, to satisfy the render call. If there is data waiting to be processed, the Remote I/O unit uses it. Otherwise, and as shown in the figure, it instead invokes the render callback of whatever your app has connected to its input. In this example, the Remote I/O unit’s input is connected to an effect unit’s output. So, the I/O unit pulls on the effect unit, asking for a slice of audio frames.</li><li>The effect unit behaves just as the Remote I/O unit did. When it needs audio data, it gets it from its input connection. In this example, the effect unit pulls on your app’s render callback function.</li><li>Your app’s render callback function is the final recipient of the pull. It supplies the requested frames to the effect unit.</li><li>The effect unit processes the slice supplied by your app’s render callback. The effect unit then supplies the processed data that were previously requested (in step 2) to the Remote I/O unit.</li><li>The Remote I/O unit processes the slice provided by the effect unit. The Remote I/O unit then supplies the processed slice originally requested (in step 1) to the virtual output device. This completes one cycle of pull.</li></ol><blockquote><p>This invocation asks for one slice of processed audio data frames.：这个调用请求一个已经处理过的音频数据帧</p><p>looks in its input buffers for audio data to process：在输入缓冲区中查找要处理的音频数据</p><p>The effect unit behaves just as the Remote I/O unit did. :effect unit 和remoteI/O Unit的表现相同。（行为/表现和remoteIO相同）</p><p>the final recipient of the pull： 拉取的最终接收者</p><p>The effect unit processes the slice supplied by your app’s render callback.： effect unit 将处理你打的app call back的数据。</p><p>This completes one cycle of pull.：完成了一个拉动周期/循环</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="render-callback-functions-feed-audio-to-audio-units"></a>Render Callback Functions Feed Audio to Audio Units<a class="hash-link" href="#render-callback-functions-feed-audio-to-audio-units" title="Direct link to heading">#</a></h3><p>To provide audio from disk or memory to an audio unit input bus, convey it using a render callback function that conforms to the AURenderCallback prototype. The audio unit input invokes your callback when it needs another slice of sample frames, as described in Audio Flows Through a Graph Using Pull.</p><blockquote><p>convey it：传输</p><p>conforms to the AURenderCallback prototype.：符合 “AURenderCallback” 原型</p></blockquote><p>The process of writing a render callback function is perhaps the most creative aspect of designing and building an audio unit application. It’s your opportunity to generate or alter sound in any way you can imagine and code.</p><blockquote><p>The process of writing a render callback function： 编写回调函数的过程 process：过程，处理</p><p>the most creative aspect of designing and building an audio unit application：designing and building an audio unit application 中最具创意的方面</p><p><strong>generate or alter sound in any way you can imagine and code：用你想象和编码的任何方式去生产和改变声音。alter。</strong></p></blockquote><p>At the same time, render callbacks have a strict performance requirement that you must adhere to. A render callback lives on a real-time priority thread on which subsequent render calls arrive asynchronously. The work you do in the body of a render callback takes place in this time-constrained environment. If your callback is still producing sample frames in response to the previous render call when the next render call arrives, you get a gap in the sound. For this reason you must not take locks, allocate memory, access the file system or a network connection, or otherwise perform time-consuming tasks in the body of a render callback function.</p><blockquote><p>strict performance requirement that you must adhere to.： 严格的性能要求你必须遵守</p><p>lives on a real-time priority thread：驻守在一个实时优先级的线程上</p><p>subsequent render calls arrive asynchronously. 后续渲染回调会异步到达</p><p>takes place in this time-constrained environment.：发生在这种时间受限的环境中</p><p>otherwise perform time-consuming tasks： 其他方式执行耗时任务</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="understanding-the-audio-unit-render-callback-function"></a>Understanding the Audio Unit Render Callback Function<a class="hash-link" href="#understanding-the-audio-unit-render-callback-function" title="Direct link to heading">#</a></h4><p>Listing 1-5 shows the header of a render callback function that conforms to the <code>AURenderCallback</code> prototype. This section describes the purpose of each of its parameters in turn and explains how to use each one.</p><p>Listing 1-5  A render callback function header</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">static OSStatus MyAURenderCallback (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    void                        *inRefCon,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    AudioUnitRenderActionFlags  *ioActionFlags,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    const AudioTimeStamp        *inTimeStamp,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32                      inBusNumber,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32                      inNumberFrames,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    AudioBufferList             *ioData</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">) { /* callback body */ }</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>The <em>inRefCon</em> parameter points to a programmatic context you specify when attaching the callback to an audio unit input (see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW13" target="_blank" rel="noopener noreferrer">Write and Attach Render Callback Functions</a>). The purpose of this context is to provide the callback function with any audio input data or state information it needs to calculate the output audio for a given render call.</p><p>The <em>ioActionFlags</em> parameter lets a callback provide a hint to the audio unit that there is no audio to process. Do this, for example, if your app is a synthetic guitar and the user is not currently playing a note. During a callback invocation for which you want to output silence, use a statement like the following in the body of the callback:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">*ioActionFlags |= kAudioUnitRenderAction_OutputIsSilence;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><blockquote><p>provide a hint to the audio unit that there is no audio to process.提供一个提示给audio unit，这里没有audio需要处理</p><p>synthetic guitar：合成吉他</p><p>the user is not currently playing a note.用户未在演奏音符</p><p>output silence静默输出</p></blockquote><p>When you want to produce silence, you must also explicitly set the buffers pointed at by the <em>ioData</em> parameter to 0. There’s more about this in the description for that parameter.</p><blockquote><p>the buffers pointed at by the <em>ioData</em> parameter <em>ioData</em>参数指向的缓冲区</p></blockquote><p>The inTimeStamp parameter represents the time at which the callback was invoked. It contains an AudioTimeStamp structure, whose mSampleTime field is a sample-frame counter. On each invocation of the callback, the value of the mSampleTime field increments by the number in the inNumberFrames parameter. If your app is a sequencer or a drum machine, for example, you can use the mSampleTime value for scheduling sounds.</p><blockquote><p>scheduling sounds 安排声音</p></blockquote><p>The inBusNumber parameter indicates the audio unit bus that invoked the callback, allowing you to branch within the callback depending on this value. In addition, when attaching a callback to an audio unit, you can specify a different context (inRefCon) for each bus.</p><blockquote><p>when attaching a callback to an audio unit 当把回调添加到音频单元时</p><p>you can specify a different context (inRefCon) for each bus.你可以为不同的总线指定不同的上下文</p></blockquote><p>The <em>inNumberFrames</em> parameter indicates the number of audio sample frames that the callback is being asked to provide on the current invocation. You provide those frames to the buffers in the <em>ioData</em> parameter.</p><p>The <em>ioData</em> parameter points to the audio data buffers that the callback must fill when it is invoked. The audio you place into these buffers must conform to the audio stream format of the bus that invoked the callback.</p><p>If you are playing silence for a particular invocation of the callback, explicitly set these buffers to 0, such as by using the <code>memset</code> function.</p><p>Figure 1-7 depicts a pair of noninterleaved stereo buffers in an <em>ioData</em> parameter. Use the elements of the figure to visualize the details of <em>ioData</em> buffers that your callback needs to fill.</p><p><strong>Figure 1-7</strong> The <code>ioData</code> buffers for a stereo render callback function<img alt="img" src="/assets/images/ioDataBuffers_2x-647f5345479e4bda63135f72f8f02fef.png"></p><blockquote><p>depicts a pair of noninterleaved stereo buffers in an <em>ioData</em> parameter. 在<em>ioData</em>参数描述了一对非交错立体声缓冲区</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="audio-stream-formats-enable-data-flow"></a>Audio Stream Formats Enable Data Flow<a class="hash-link" href="#audio-stream-formats-enable-data-flow" title="Direct link to heading">#</a></h3><p>When working with audio data at the individual sample level, as you are when using audio units, it’s not enough to specify the correct data type to represent the audio. The layout of the bits in a single audio sample value has meaning, so a data type like Float32 or UInt16 is not expressive enough. In this section you learn about Core Audio’s solution to this problem.</p><blockquote><p>The layout of the bits 位布局</p><p>not expressive enough.表现力不够</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="working-with-the-audiostreambasicdescription-structure"></a>Working with the AudioStreamBasicDescription structure<a class="hash-link" href="#working-with-the-audiostreambasicdescription-structure" title="Direct link to heading">#</a></h4><p>The currency for moving audio values around in your app, and between your app and audio hardware, is the <code>AudioStreamBasicDescription</code> structure, shown in Listing 1-6 and described fully in <em><a href="https://developer.apple.com/documentation/coreaudio/core_audio_data_types" target="_blank" rel="noopener noreferrer">Core Audio Data Types Reference</a></em>.</p><p>Listing 1-6  The AudioStreamBasicDescription structure</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">struct</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(255, 203, 107)">AudioStreamBasicDescription</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    Float64 mSampleRate</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mFormatID</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mFormatFlags</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mBytesPerPacket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mFramesPerPacket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mBytesPerFrame</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mChannelsPerFrame</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mBitsPerChannel</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32  mReserved</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">typedef</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">struct</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(255, 203, 107)">AudioStreamBasicDescription</span><span class="token plain">  AudioStreamBasicDescription</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Because the name <code>AudioStreamBasicDescription</code> is long, it’s often abbreviated in conversation and documentation as <em>ASBD</em>. To define values for the fields of an ASBD, write code similar to that shown in Listing 1-7.</p><p>Listing 1-7  Defining an ASBD for a stereo stream</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">size_t bytesPerSample </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">sizeof</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">AudioUnitSampleType</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioStreamBasicDescription stereoStreamFormat </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFormatID          </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> kAudioFormatLinearPCM</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFormatFlags       </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> kAudioFormatFlagsAudioUnitCanonical</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBytesPerPacket    </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> bytesPerSample</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBytesPerFrame     </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> bytesPerSample</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFramesPerPacket   </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBitsPerChannel    </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">8</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> bytesPerSample</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mChannelsPerFrame  </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain">           </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// 2 indicates stereo</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stereoStreamFormat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mSampleRate        </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> graphSampleRate</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>To start, determine the data type to represent one audio sample value. This example uses the <code>AudioUnitSampleType</code> defined type, the recommended data type for most audio units. In iOS, <code>AudioUnitSampleType</code> is defined to be an 8.24 fixed-point integer. The first line in Listing 1-7 calculates the number of bytes in the type; that number is required when defining some of the field values of an ASBD, as you can see in the listing.</p><p>Next, still referring to Listing 1-7, declare a variable of type <code>AudioStreamBasicDescription</code> and initialize its fields to 0 to ensure that no fields contain garbage data. Do not skip this zeroing step; if you do, you are certain to run into trouble later.</p><p>Now define the ASBD field values. Specify <code>kAudioFormatLinearPCM</code> for the <em>mFormatID</em> field. Audio units use uncompressed audio data, so this is the correct format identifier to use whenever you work with audio units.</p><p>Next, for most audio units, specify the <code>kAudioFormatFlagsAudioUnitCanonical</code> metaflag for the <em>mFormatFlags</em> field. This flag is defined in <code>CoreAudio.framework/CoreAudioTypes.h</code> as follows:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">kAudioFormatFlagsAudioUnitCanonical </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> kAudioFormatFlagIsFloat </span><span class="token operator" style="color:rgb(137, 221, 255)">|</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                kAudioFormatFlagsNativeEndian </span><span class="token operator" style="color:rgb(137, 221, 255)">|</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                     kAudioFormatFlagIsPacked </span><span class="token operator" style="color:rgb(137, 221, 255)">|</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                             kAudioFormatFlagIsNonInterleaved</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>This metaflag takes care of specifying all of the layout details for the bits in a linear PCM sample value of type <code>AudioUnitSampleType</code>.</p><p>Certain audio units employ an atypical audio data format, requiring a different data type for samples and a different set of flags for the <em>mFormatFlags</em> field. For example, the 3D Mixer unit requires the <code>UInt16</code> data type for its audio sample values and requires the ASBD’s <em>mFormatFlags</em> field to be set to <code>kAudioFormatFlagsCanonical</code>. When working with a particular audio unit, be careful to use the correct data format and the correct format flags. (See <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a>.)</p><blockquote><p>be careful to use the correct data format and the correct format flags.小心使用正确的数据格式和正确的格式标识</p><p>Certain audio units employ an atypical audio data format, 特定的audiounit 需要采用特定的音频数据格式</p></blockquote><p>Continuing through <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW32" target="_blank" rel="noopener noreferrer">Listing 1-7</a>, the next four fields further specify the organization and meaning of the bits in a sample frame. Set these fields—<em>mBytesPerPacket</em>, <em>mBytesPerFrame</em>, <em>mFramesPerPacket</em>, and <em>mBitsPerChannel</em> fields—according to the nature of the audio stream you are using. To learn the meaning of each of these fields, refer to the documentation for the <code>AudioStreamBasicDescription</code> structure. You can see an example of filled-out ASBDs in the sample code project <em>Audio Mixer (MixerHost)</em>.</p><p>Set the ASBD’s <em>mChannelsPerFrame</em> field according to the number of channels in the stream—1 for mono audio, 2 for stereo, and so on.</p><p>Finally, set the <em>mSampleRate</em> field according to the sample rate that you are using throughout your app. <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW34" target="_blank" rel="noopener noreferrer">Understanding Where and How to Set Stream Formats</a> explains the importance of avoiding sample rate conversions. <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW9" target="_blank" rel="noopener noreferrer">Configure Your Audio Session</a> explains how to ensure that your application’s sample rate matches the audio hardware sample rate.</p><blockquote><p>avoiding sample rate conversions 避免采样率转换</p></blockquote><p>Rather than specify an ASBD field by field as you’ve seen here, you can use the C++ utility methods provided in the <code>CAStreamBasicDescription.h</code> file (<code>/Developer/Extras/CoreAudio/PublicUtility/</code>). In particular, view the <code>SetAUCanonical</code> and <code>SetCanonical</code> C++ methods. These specify the correct way to derive ASBD field values given three factors:</p><ul><li>Whether the stream is for I/O (<code>SetCanonical</code>) or for audio processing (<code>SetAUCanonical</code>)</li><li>How many channels you want the stream format to represent</li><li>Whether you want the stream format interleaved or noninterleaved</li></ul><blockquote><p>Rather than ... you can 你可以...而不必</p><p>Whether you want the stream format interleaved or noninterleaved 您是要流格式是交错还是非交错</p></blockquote><p>Whether or not you include the <code>CAStreamBasicDescription.h</code> file in your project to use its methods directly, Apple recommends that you study that file to learn the correct way to work with an <code>AudioStreamBasicDescription</code> structure.</p><p>See <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW29" target="_blank" rel="noopener noreferrer">Troubleshooting Tips</a> for ideas on how to fix problems related to audio data stream formats.</p><p>关于ASBD的理解可以参考：<a href="https://www.cnblogs.com/huahuahu/p/iOS-yin-pin-kai-fa-zhiAudioStreamBasicDescription.html" target="_blank" rel="noopener noreferrer">https://www.cnblogs.com/huahuahu/p/iOS-yin-pin-kai-fa-zhiAudioStreamBasicDescription.html</a></p><p>首先，音频文件的产生是模拟信号-&gt;PCM以后的数字信号-&gt;压缩、编码以后的音频文件。
PCM时采样频率叫做sample rate。
每一次采样可以得到若干采样数据，对应多个channel。
每一个采样点得到的若干采样数据组合起来，叫做一个frame。
若干frame组合起来叫做一个packet。
<code>mSampleRate</code>，就是采用频率
<code>mBitsPerChannel</code>，就是每个采样数据的位数
<code>mChannelsPerFrame</code>,可以理解为声道数，也就是一个采样时刻产生几个采样数据。
<code>mFramesPerPacket</code>，就是每个packet的中frame的个数，等于这个packet中经历了几次采样间隔。
<code>mBytesPerPacket</code>，每个packet中数据的字节数。
<code>mBytesPerFrame</code>，每个frame中数据的字节数</p><ul><li>计算公式</li></ul><ol><li><p>计算每个packet的持续时间</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">duration = (1 / mSampleRate) * mFramesPerPacket</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div></li><li><p>计算<code>mBitsPerChannel</code>
对于<code>kAudioFormatFlagsCanonical</code>的PCM数据，有以下公式。大概每个采样数据的字节数用<code>AudioSampleType</code>描述。</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">mBitsPerChannel = 8 * sizeof (AudioSampleType);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div></li><li><p>计算<code>mBytesPerFrame</code></p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">mBytesPerFrame = n * sizeof (AudioSampleType);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>其中n是声道数目</p></li></ol><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="understanding-where-and-how-to-set-stream-formats"></a>Understanding Where and How to Set Stream Formats<a class="hash-link" href="#understanding-where-and-how-to-set-stream-formats" title="Direct link to heading">#</a></h4><p>You must set the audio data stream format at critical points in an audio processing graph. At other points, the system sets the format. At still other points, audio unit connections propagate a stream format from one audio unit to another.</p><blockquote><p>at critical points 关键点</p><p>connections propagate 连接传播</p></blockquote><p>The audio input and output hardware on an iOS device have system-determined audio stream formats. These formats are always uncompressed, in linear PCM format, and interleaved. The system imposes these formats on the outward-facing sides of the I/O unit in an audio processing graph, as depicted in Figure 1-8.</p><blockquote><p>imposes these formats on the outward-facing sides of the I/O unit 将这些格式强加到I/OUnit的外侧面</p></blockquote><p><strong>Figure 1-8</strong> Where to set audio data stream formats<img alt="img" src="/assets/images/IOWithoutRenderCallback_2x-9eca9b2c6d20ac1d3b75c7b99eef2396.png"></p><p>In the figure, the microphone represents the input audio hardware. The system determines the input hardware’s audio stream format and imposes it onto the input scope of the Remote I/O unit’s input element.</p><p>Similarly, the loudspeakers in the figure represent the output audio hardware. The system determines the output hardware’s stream format and imposes it onto the output scope of the Remote I/O unit’s output element.</p><p>Your application is responsible for establishing the audio stream formats on the inward-facing sides of the I/O unit’s elements. The I/O unit performs any necessary conversion between your application formats and the hardware formats. Your application is also responsible for setting stream formats wherever else they are required in a graph. In some cases, such as at the output of the Multichannel Mixer unit in Figure 1-8, you need to set only a portion of the format—specifically, the sample rate. <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW2" target="_blank" rel="noopener noreferrer">Start by Choosing a Design Pattern</a> shows you where to set stream formats for various types of audio unit apps. <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a> lists the stream format requirements for each iOS audio unit.</p><blockquote><p>is responsible for 负责</p><p>the inward-facing sides of the I/O unit’s elements.  内向侧面</p></blockquote><p>A key feature of an audio unit connection, as shown in Figure 1-8, is that the connection propagates the audio data stream format from the output of its source audio unit to the input of its destination audio unit. This is a critical point so it bears emphasizing: Stream format propagation takes place by way of an audio unit connection and in one direction only—from the output of a source audio unit to an input of a destination audio unit.</p><blockquote><p>A key feature 一个关键功能</p><p>This is a critical point 这是一个关键点</p><p>so it bears emphasizing 因此需要强调</p><p>Stream format propagation takes place by way of an audio unit connection：流格式传播是通过audiounit 连接进行的</p></blockquote><p>Take advantage of format propagation. It can significantly reduce the amount of code you need to write. For example, when connecting the output of a Multichannel Mixer unit to the Remote I/O unit for playback, you do not need to set the stream format for the I/O unit. It is set appropriately by the connection between the audio units, based on the output stream format of the mixer (see Figure 1-8).</p><blockquote><p>format propagation 格式传播</p></blockquote><p>Stream format propagation takes place at one particular point in an audio processing graph’s life cycle—namely, upon initialization. See Initialize and Start the Audio Processing Graph.</p><blockquote><p>Stream format propagation 流格式传播</p><p>namely, upon initialization.即 初始化时</p></blockquote><p>You have great flexibility in defining your application audio stream formats. However, whenever possible, use the sample rate that the hardware is using. When you do, the I/O unit need not perform sample rate conversion. This minimizes energy usage—an important consideration in a mobile device—and maximizes audio quality. To learn about working with the hardware sample rate, see Configure Your Audio Session.</p><blockquote><p>maximizes audio quality 音频质量最大化</p></blockquote><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="constructing-audio-unit-apps"></a>Constructing Audio Unit Apps<a class="hash-link" href="#constructing-audio-unit-apps" title="Direct link to heading">#</a></h2><p>Now that you understand how audio unit hosting works, as explained in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW11" target="_blank" rel="noopener noreferrer">Audio Unit Hosting Fundamentals</a>, you are well prepared to build the audio unit portion of your app. The main steps are choosing a design pattern and then writing the code to implement that pattern.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="start-by-choosing-a-design-pattern"></a>Start by Choosing a Design Pattern<a class="hash-link" href="#start-by-choosing-a-design-pattern" title="Direct link to heading">#</a></h3><p>There are a half dozen basic design patterns for hosting audio units in an iOS app. Begin by picking the one that most closely represents what you want your app to do with audio. As you learn each pattern, notice the common features. Every pattern:</p><blockquote><p>a half dozen ： 6个（ 一打（dozen）的一半）</p></blockquote><ul><li>Has exactly one I/O unit.</li><li>Uses a single audio stream format throughout the audio processing graph—although there can be variations on that format, such as mono and stereo streams feeding a mixer unit.</li><li>Requires that you set the stream format, or portions of the stream format, at specific locations.</li></ul><blockquote><p>at specific locations： 在特定的位置</p></blockquote><p>Setting stream formats correctly is essential to establishing audio data flow. Most of these patterns rely on automatic propagation of audio stream formats from source to destination, as provided by an audio unit connection. Take advantage of this propagation when you can because it reduces the amount of code to write and maintain. At the same time, be sure that you understand where it is required for you to set stream formats. For example, you must set the full stream format on the input <em>and</em> output of an iPod EQ unit. Refer to the usage tables in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a> for all iOS audio unit stream format requirements.</p><blockquote><p>is essential to 基本的至关重要的</p><p>automatic propagation：自动传播</p><p>the full stream format：完整流格式</p><p>the usage tables ：用法表</p></blockquote><p>In most cases, the design patterns in this chapter employ an audio processing graph (of type AUGraph). You could implement any one of these patterns without using a graph, but using one simplifies the code and supports dynamic reconfiguration, as described in Audio Processing Graphs Manage Audio Units.</p><blockquote><p>employ an audio processing graph 使用audio unit 图</p><p>using one simplifies the code and supports dynamic reconfiguration, 使用一种简化代码和支持动态配置</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="io-pass-through"></a>I/O Pass Through<a class="hash-link" href="#io-pass-through" title="Direct link to heading">#</a></h4><blockquote><p>I/O直通</p></blockquote><p>The I/O pass-through pattern sends incoming audio directly to the output hardware, with no option to work with the audio data. Although this isn’t of much practical value, building an audio unit hosting app based on this pattern is a good way to verify and cement your understanding of audio unit concepts. Figure 2-1 illustrates this pattern.</p><blockquote><p>verify and cement your understanding of audio unit concepts: 验证和巩固您对audio unit 的理解</p></blockquote><p><strong>Figure 2-1</strong> Simultaneous I/O pass through<img alt="img" src="/assets/images/IOPassThrough_2x-d2931e117d8b9548f67f9c47d2ceb7c4.png"></p><p>As you can see in the figure, the audio input hardware imposes its stream format on the outward-facing side of the Remote I/O unit’s input element. You, in turn, specify the format that you want to use on the inward-facing side of this element. The audio unit performs format conversion as needed. To avoid unnecessary sample rate conversion, be sure to use the audio hardware sample rate when defining your stream format.</p><p>The input element is disabled by default, so be sure to enable it; otherwise, audio cannot flow.</p><p>The pattern shown in Figure 2-1 takes advantage of the audio unit connection between the two Remote I/O elements. Specifically, you do not set a stream format on the input scope of the audio unit’s output element. The connection propagates the format you specified for the input element.</p><blockquote><p>The connection propagates the format you specified for the input element. 连接传播（您为输入元素指定的）格式</p></blockquote><p>The outward-facing side of the output element takes on the audio output hardware’s stream format, and the output element performs format conversion for the outgoing audio as needed.</p><blockquote><p>as needed. 根据需要</p></blockquote><p>Using this pattern, you need not configure any audio data buffers.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="io-without-a-render-callback-function"></a>I/O Without a Render Callback Function<a class="hash-link" href="#io-without-a-render-callback-function" title="Direct link to heading">#</a></h4><p>Adding one or more other audio units between the Remote I/O unit’s elements lets you construct a more interesting app. For example, you could use a Multichannel Mixer unit to position the incoming microphone audio in a stereo field or to provide output volume control. In this design pattern, there is still no callback function in play, as shown in Figure 2-2. This simplifies the pattern but limits its utility. Without a render callback function you don’t have a means to manipulate the audio directly.</p><blockquote><p>position the incoming microphone audio 放置麦克风的输入音频</p><p>This simplifies the pattern 这简化了模式</p><p>limits its utility 限制了功能</p></blockquote><p><strong>Figure 2-2</strong> Simultaneous I/O without a render callback function<img alt="img" src="/assets/images/IOWithoutRenderCallback_2x-1375043-9eca9b2c6d20ac1d3b75c7b99eef2396.png"></p><p>In this pattern, you configure both elements of the Remote I/O unit just as you do in the pass-through pattern. To set up the Multichannel Mixer unit, you must set the sample rate of your stream format on the mixer output, as indicated in Figure 2-2.</p><p>The mixer’s input stream format is established automatically by propagation from the output of the Remote I/O unit’s input element, by way of the audio unit connection. Similarly, the stream format for the input scope of the Remote I/O unit’s output element is established by the audio unit connection, thanks to propagation from the mixer unit output.</p><p>In any instance of this pattern—indeed, whenever you use other audio units in addition to an I/O unit—you must set the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property as described in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_properties" target="_blank" rel="noopener noreferrer">Audio Unit Properties Reference</a></em>.</p><p>As with the pass-through pattern, you need not configure any audio data buffers.</p><p>理解：连接输入源(这里是麦克风) Remote I/O unit 的Element 是 Input Element 连接输出的 Element 是output Element。 每个element 有分为input scope 和 output scope。</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="io-with-a-render-callback-function"></a>I/O with a Render Callback Function<a class="hash-link" href="#io-with-a-render-callback-function" title="Direct link to heading">#</a></h4><p>By placing a render callback function between the input and output elements of a Remote I/O unit, you can manipulate incoming audio before it reaches the output hardware. In a very simple case, you could use the render callback function to adjust output volume. However, you could add tremolo, ring-modulation, echo, or other effects. By making use of the Fourier transforms and convolution functions available in the Accelerate framework (see <em><a href="https://developer.apple.com/documentation/accelerate" target="_blank" rel="noopener noreferrer">Accelerate Framework Reference</a></em>), your possibilities are endless. This pattern is depicted in Figure 2-3.</p><blockquote><p>However, you could add tremolo, ring-modulation, echo, or other effects. 你可以添加颤音，振铃调制，回声和其他效果</p><p>By making use of the Fourier transforms and convolution functions available in the Accelerate framework (see <em><a href="https://developer.apple.com/documentation/accelerate" target="_blank" rel="noopener noreferrer">Accelerate Framework Reference</a></em>), your possibilities are endless.  通过利用（Accelerate框架中可用的）Fourier变换和卷积函数（请参阅Accelerate Framework Reference），您的可能性是无限的。</p></blockquote><p><strong>Figure 2-3</strong> Simultaneous I/O with a render callback function<img alt="img" src="/assets/images/IOWithRenderCallback_2x-bc85de3586681d29eb9e572ed7450ed6.png"></p><p>As you can see in the figure, this pattern uses both elements of the Remote I/O unit, as in the previous patterns in this chapter. Attach your render callback function to the input scope of the output element. When that element needs another set of audio sample values, it invokes your callback. Your callback, in turn, obtains fresh samples by invoking the render callback function of the Remote I/O unit’s input element.</p><p>Just as for the other I/O patterns, you must explicitly enable input on the Remote I/O unit, because input is disabled by default. And, as for the other I/O patterns, you need not configure any audio data buffers.</p><p>Notice that when you establish an audio path from one audio unit to another using a render callback function, as in this pattern, the callback takes the place of an audio unit connection.</p><blockquote><p>the callback takes the place of an audio unit connection. 回调将取代音频连接</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="output-only-with-a-render-callback-function"></a>Output-Only with a Render Callback Function<a class="hash-link" href="#output-only-with-a-render-callback-function" title="Direct link to heading">#</a></h4><p>Choose this pattern for musical games and synthesizers—apps for which you are generating sounds and need maximum responsiveness. At its simplest, this pattern involves one render callback function connected directly to the input scope of a Remote I/O unit’s output element, as shown in Figure 2-4.</p><blockquote><p>musical games and synthesizers 声音游戏和合成器</p><p>need maximum responsiveness 需要最大响应度</p></blockquote><p><strong>Figure 2-4</strong> Output-only with a render callback function<img alt="img" src="/assets/images/OutputOnlyWithRenderCallback_2x-6f167429b43f441a972f7f09e6ba7650.png"></p><p>You can use this same pattern to build an app with a more complex audio structure. For example, you might want to generate several sounds, mix them together, and then play them through the device’s output hardware. Figure 2-5 shows such a case. Here, the pattern employs an audio processing graph and two additional audio units, a Multichannel Mixer and an iPod EQ.</p><p><strong>Figure 2-5</strong> A more complex example of output-only with a render callback function<img alt="img" src="/assets/images/OutputOnlyWithRenderCallbackExtended_2x-c2507a0470067ed79d85d31dd00071cb.png"></p><p>In the figure, notice that the iPod EQ requires you to set your full stream format on both input and output. The Multichannel Mixer, on the other hand, needs only the correct sample rate to be set on its output. The full stream format is then propagated by the audio unit connection from the mixer’s output to the input scope of the Remote I/O unit’s output element. These usage details, and other specifics of using the various iOS audio units, are described in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a>.</p><p>For each of the Multichannel Mixer unit inputs, as you see in Figure 2-5, the full stream format is set. For input 0, you set it explicitly. For input 1, the format is propagated by the audio unit connection from the output of the iPod EQ unit. In general, you must account for the stream-format needs of each audio unit individually.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="other-audio-unit-hosting-design-patterns"></a>Other Audio Unit Hosting Design Patterns<a class="hash-link" href="#other-audio-unit-hosting-design-patterns" title="Direct link to heading">#</a></h4><p>There are two other main design patterns for audio units hosting. To record or analyze audio, create an input-only app with a render callback function. The callback function is invoked by your application, and it in turn invokes the render method of the Remote I/O unit’s input element. However, in most cases, a better choice for an app like this is to use an input audio queue object (of type <code>AudioQueueRef</code> instantiated using the <code>AudioQueueNewInput</code> function), as explained in <em><a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343" target="_blank" rel="noopener noreferrer">Audio Queue Services Programming Guide</a></em>. Using an audio queue object provides a great deal more flexibility because its render callback function is not on a realtime thread.</p><p>To perform offline audio processing, use a Generic Output unit. Unlike the Remote I/O unit, this audio unit does not connect to the device’s audio hardware. When you use it to send audio to your application, it depends on your application to invoke its render method.</p><blockquote><p>音频单元托管还有其他两种主要设计模式。 要录制或分析音频，请创建具有渲染回调功能的仅输入应用程序。 回调函数由您的应用程序调用，并依次调用远程I / O单元的input元素的render方法。 但是，在大多数情况下，对于像这样的应用程序，更好的选择是使用输入音频队列对象（使用AudioQueueNewInput函数实例化的AudioQueueRef类型），如《音频队列服务编程指南》中所述。 使用音频队列对象可提供更大的灵活性，因为其渲染回调函数不在实时线程上。</p><p>要执行离线音频处理，请使用通用输出单元。 与远程I / O单元不同，此音频单元未连接到设备的音频硬件。 当您使用它向应用程序发送音频时，它取决于您的应用程序调用其render方法。</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="constructing-your-app"></a>Constructing Your App<a class="hash-link" href="#constructing-your-app" title="Direct link to heading">#</a></h3><p>No matter which design pattern you choose, the steps for constructing an audio unit hosting app are basically the same:</p><ol><li>Configure your audio session.</li><li>Specify audio units.</li><li>Create an audio processing graph, then obtain the audio units.</li><li>Configure the audio units.</li><li>Connect the audio unit nodes.</li><li>Provide a user interface.</li><li>Initialize and then start the audio processing graph.</li></ol><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="configure-your-audio-session"></a>Configure Your Audio Session<a class="hash-link" href="#configure-your-audio-session" title="Direct link to heading">#</a></h4><p>The first step in building an audio unit application is the same step as for any iOS audio application: You configure the audio session. The characteristics of the audio session largely determine your app’s audio capabilities as well as its interactivity with the rest of the system. Start by specifying the sample rate you want to use in your application, as shown here:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">self.graphSampleRate = 44100.0; // Hertz</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><blockquote><p>The characteristics of the audio session。 audio session的特性</p></blockquote><p>Next, employ the audio session object to request that the system use your preferred sample rate as the device hardware sample rate, as shown in Listing 2-1. The intent here is to avoid sample rate conversion between the hardware and your app. This maximizes CPU performance and sound quality, and minimizes battery drain.</p><blockquote><p>The intent here is 这个的目的就是</p><p>maximizes CPU performance and sound quality:最大化CPU性能，和声音质量</p><p>minimizes battery drain 最小化电池损耗</p></blockquote><p>Listing 2-1  Configuring an audio session</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">NSError *audioSessionError = nil;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AVAudioSession *mySession = [AVAudioSession sharedInstance];     // 1</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">[mySession setPreferredHardwareSampleRate: graphSampleRate       // 2</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                    error: &amp;audioSessionError];</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">[mySession setCategory: AVAudioSessionCategoryPlayAndRecord      // 3</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                    error: &amp;audioSessionError];</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">[mySession setActive: YES                                        // 4</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">               error: &amp;audioSessionError];</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">self.graphSampleRate = [mySession currentHardwareSampleRate];    // 5</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>The preceding lines do the following:</p><ol><li>Obtain a reference to the singleton audio session object for your application.</li><li>Request a hardware sample rate. The system may or may not be able to grant the request, depending on other audio activity on the device.</li><li>Request the audio session category you want. The “play and record” category, specified here, supports audio input and output.</li><li>Request activation of your audio session.</li><li>After audio session activation, update your own sample rate variable according to the actual sample rate provided by the system.</li></ol><blockquote><p>The system may or may not be able to grant the request 系统可能不会批准您的请求</p></blockquote><p>There’s one other hardware characteristic you may want to configure: audio hardware I/O buffer duration. The default duration is about 23 ms at a 44.1 kHz sample rate, equivalent to a slice size of 1,024 samples. If I/O latency is critical in your app, you can request a smaller duration, down to about 0.005 ms (equivalent to 256 samples), as shown here:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">self.ioBufferDuration = 0.005;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">[mySession setPreferredIOBufferDuration: ioBufferDuration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                                  error: &amp;audioSessionError];</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>For a complete explanation of how to configure and use the audio session object, see <em><a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875" target="_blank" rel="noopener noreferrer">Audio Session Programming Guide</a></em>.</p><blockquote><p>equivalent to : 相当于</p><p>If I/O latency is critical in your app： 如果IO延迟在你APP中很关键 （critical  挑剔的决定性的）</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="specify-the-audio-units-you-want"></a>Specify the Audio Units You Want<a class="hash-link" href="#specify-the-audio-units-you-want" title="Direct link to heading">#</a></h4><p>At runtime, after your audio session configuration code has run, your app has not yet acquired audio units. You specify each one that you want by using an <code>AudioComponentDescription</code> structure. See <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW19" target="_blank" rel="noopener noreferrer">Use Identifiers to Specify and Obtain Audio Units</a> for how to do this. The identifier keys for each iOS audio unit are listed in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW14" target="_blank" rel="noopener noreferrer">Identifier Keys for Audio Units</a>.</p><p>Audio unit specifiers in hand, you then build an audio processing graph according to the pattern you’ve picked.</p><blockquote><p>your app has not yet acquired audio units : 你的应用尚未获取到audio units</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="build-an-audio-processing-graph"></a>Build an Audio Processing Graph<a class="hash-link" href="#build-an-audio-processing-graph" title="Direct link to heading">#</a></h4><p>In this step, you create the skeleton of one of the design patterns explained in the first part of this chapter. Specifically, you:</p><ol><li>Instantiate an <code>AUGraph</code> opaque type. The instance represents the audio processing graph.</li><li>Instantiate one or more <code>AUNode</code> opaque types, each of which represents one audio unit in the graph.</li><li>Add the nodes to the graph.</li><li>Open the graph and instantiate the audio units.</li><li>Obtain references to the audio units.</li></ol><p>Listing 2-2 shows how to perform these steps for a graph that contains a Remote I/O unit and a Multichannel Mixer unit. It assumes you’ve already defined an <code>AudioComponentDescription</code> structure for each of these audio units.</p><p><strong>Listing 2-2</strong> Building an audio processing graph</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraph processingGraph;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">NewAUGraph (&amp;processingGraph);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUNode ioNode;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUNode mixerNode;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphAddNode (processingGraph, &amp;ioUnitDesc, &amp;ioNode);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphAddNode (processingGraph, &amp;mixerDesc, &amp;mixerNode);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>The <code>AUGraphAddNode</code> function calls make use of the audio unit specifiers <em>ioUnitDesc</em> and <em>mixerDesc</em>. At this point, the graph is instantiated and owns the nodes that you’ll use in your app. To open the graph and instantiate the audio units, call <code>AUGraphOpen</code>:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphOpen (processingGraph);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Then, obtain references to the audio unit instances by way of the <code>AUGraphNodeInfo</code> function, as shown here:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnit ioUnit;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnit mixerUnit;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphNodeInfo (processingGraph, ioNode, NULL, &amp;ioUnit);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphNodeInfo (processingGraph, mixerNode, NULL, &amp;mixerUnit);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>The <code>ioUnit</code> and <code>mixerUnit</code> variables now hold references to the audio unit instances in the graph, allowing you to configure and then interconnect the audio units.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="configure-the-audio-units"></a>Configure the Audio Units<a class="hash-link" href="#configure-the-audio-units" title="Direct link to heading">#</a></h4><p>Each iOS audio unit requires its own configuration, as described in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a>. However, some configurations are common enough that all iOS audio developers should be familiar with them.</p><p>The Remote I/O unit, by default, has output enabled and input disabled. If your app performs simultaneous I/O, or uses input only, you must reconfigure the I/O unit accordingly. For details, see the <code>kAudioOutputUnitProperty_EnableIO</code> property in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_properties" target="_blank" rel="noopener noreferrer">Audio Unit Properties Reference</a></em>.</p><p>All iOS audio units, with the exception of the Remote I/O and Voice-Processing I/O units, need their kAudioUnitProperty_MaximumFramesPerSlice property configured. This property ensures that the audio unit is prepared to produce a sufficient number of frames of audio data in response to a render call. For details, see kAudioUnitProperty_MaximumFramesPerSlice in Audio Unit Properties Reference.</p><blockquote><p>prepared to produce a sufficient number of frames of audio data 准备生产充足数量的音频数据帧</p></blockquote><p>All audio units need their audio stream format defined on input, output, or both. For an explanation of audio stream formats, see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW40" target="_blank" rel="noopener noreferrer">Audio Stream Formats Enable Data Flow</a>. For the specific stream format requirements of the various iOS audio units, see <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a>.</p><blockquote><p>For an explanation of audio stream formats ，see 关于“audio stream formats” 的解释，参见</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="write-and-attach-render-callback-functions"></a>Write and Attach Render Callback Functions<a class="hash-link" href="#write-and-attach-render-callback-functions" title="Direct link to heading">#</a></h4><p>For design patterns that employ render callback functions, you must write those functions and then attach them at the correct points. <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW27" target="_blank" rel="noopener noreferrer">Render Callback Functions Feed Audio to Audio Units</a> describes what these callbacks do and explains how they work. For examples of working callbacks, view the various audio unit sample code projects in the iOS Reference Library including <em>Audio Mixer (MixerHost)</em> and <em><a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770" target="_blank" rel="noopener noreferrer">aurioTouch</a></em>, and <em>SynthHost</em>.</p><p>When audio is not flowing, you can attach a render callback immediately by using the audio unit API, as shown in Listing 2-3.</p><p><strong>Listing 2-3</strong> Attaching a render callback immediately</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AURenderCallbackStruct callbackStruct;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">callbackStruct.inputProc        = &amp;renderCallback;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">callbackStruct.inputProcRefCon  = soundStructArray;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitSetProperty (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    myIOUnit,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitProperty_SetRenderCallback,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitScope_Input,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    0,                 // output element</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    &amp;callbackStruct,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    sizeof (callbackStruct)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>You can attach a render callback in a thread-safe manner, even when audio is flowing, by using the audio processing graph API. Listing 2-4 shows how.</p><p><strong>Listing 2-4</strong> Attaching a render callback in a thread-safe manner</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AURenderCallbackStruct callbackStruct</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">callbackStruct</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">inputProc        </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">&amp;</span><span class="token plain">renderCallback</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">callbackStruct</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">inputProcRefCon  </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> soundStructArray</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphSetNodeInputCallback </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    processingGraph</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    myIONode</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">                 </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// output element</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token operator" style="color:rgb(137, 221, 255)">&amp;</span><span class="token plain">callbackStruct</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// ... some time later</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">Boolean graphUpdated</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphUpdate </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">processingGraph</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">&amp;</span><span class="token plain">graphUpdated</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="connect-the-audio-unit-nodes"></a>Connect the Audio Unit Nodes<a class="hash-link" href="#connect-the-audio-unit-nodes" title="Direct link to heading">#</a></h4><p>In most cases, it’s best—and easier—to establish or break connections between audio units using the <code>AUGraphConnectNodeInput</code> and <code>AUGraphDisconnectNodeInput</code> functions in the audio processing graph API. These functions are thread-safe and avoid the coding overhead of defining connections explicitly, as you must do when not using a graph.</p><p>Listing 2-5 shows how to connect the output of a mixer node to the input of an I/O unit output element using the audio processing graph API.</p><p><strong>Listing 2-5</strong> Connecting two audio unit nodes using the audio processing graph API</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitElement mixerUnitOutputBus  = 0;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitElement ioUnitOutputElement = 0;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphConnectNodeInput (</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    processingGraph,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    mixerNode,           // source node</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    mixerUnitOutputBus,  // source node bus</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    iONode,              // destination node</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    ioUnitOutputElement  // desinatation node element</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>You can, alternatively, establish and break connections between audio units directly by using the audio unit property mechanism. To do so, use the <code>AudioUnitSetProperty</code> function along with the <code>kAudioUnitProperty_MakeConnection</code> property, as shown in Listing 2-6. This approach requires that you define an <code>AudioUnitConnection</code> structure for each connection to serve as its property value.</p><p><strong>Listing 2-6</strong> Connecting two audio units directly</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitElement mixerUnitOutputBus  </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitElement ioUnitOutputElement </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitConnection mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">sourceAudioUnit    </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> mixerUnitInstance</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">sourceOutputNumber </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> mixerUnitOutputBus</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">destInputNumber    </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ioUnitOutputElement</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioUnitSetProperty </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    ioUnitInstance</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">                     </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// connection destination</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitProperty_MakeConnection</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// property key</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    kAudioUnitScope_Input</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">              </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// destination scope</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    ioUnitOutputElement</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">                </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// destination element</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token operator" style="color:rgb(137, 221, 255)">&amp;</span><span class="token plain">mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">                </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">// connection definition</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">sizeof</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">mixerOutToIoUnitIn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="provide-a-user-interface"></a>Provide a User Interface<a class="hash-link" href="#provide-a-user-interface" title="Direct link to heading">#</a></h4><p>At this point in constructing your app, the audio units—and, typically, the audio processing graph—are fully built and configured. In many cases, you’ll then want to provide a user interface to let your users fine-tune the audio behavior. You tailor the user interface to allow the user to adjust specific audio unit parameters and, in some unusual cases, audio unit properties. In either case, the user interface should also provide visual feedback regarding the current settings.</p><p><a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW21" target="_blank" rel="noopener noreferrer">Use Parameters and UIKit to Give Users Control</a> explains the basics of constructing a user interface to let a user control a parameter value. For a working example, view the sample code project <em>Audio Mixer (MixerHost)</em>.</p><p>The iPod EQ unit is one of the unusual cases in that, to change its active equalization curve, you change the value of the <code>kAudioUnitProperty_PresentPreset</code> property. You can do this whether or not audio is running. For a working example, view the sample code project <em><a href="https://developer.apple.com/library/archive/samplecode/iPhoneMixerEQGraphTest/Introduction/Intro.html#//apple_ref/doc/uid/DTS40009555" target="_blank" rel="noopener noreferrer">Mixer iPodEQ AUGraph Test</a></em>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="initialize-and-start-the-audio-processing-graph"></a>Initialize and Start the Audio Processing Graph<a class="hash-link" href="#initialize-and-start-the-audio-processing-graph" title="Direct link to heading">#</a></h4><p>Before you can start audio flow, an audio processing graph must be initialized by calling the <code>AUGraphInitialize</code> function. This critical step:</p><ul><li>Initializes the audio units owned by the graph by automatically invoking the <code>AudioUnitInitialize</code> function individually for each one. (If you were to construct a processing chain without using a graph, you would have to explicitly initialize each audio unit in turn.)</li><li>Validates the graph’s connections and audio data stream formats.</li><li>Propagates stream formats across audio unit connections.</li></ul><p>Listing 2-7 shows how to use <code>AUGraphInitialize</code>.</p><p><strong>Listing 2-7</strong> Initializing and starting an audio processing graph</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">OSStatus result = AUGraphInitialize (processingGraph);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Check for error. On successful initialization, start the graph...</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphStart (processingGraph);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Some time later</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">AUGraphStop (processingGraph);</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><blockquote><p>This critical step 关键步骤</p><p>Validates the graph’s connections and audio data stream formats. 验证图的连接和音频数据格式</p><p>Propagates stream formats across audio unit connections. 在audio unit 连接之间传播流格式</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="troubleshooting-tips"></a>Troubleshooting Tips<a class="hash-link" href="#troubleshooting-tips" title="Direct link to heading">#</a></h3><p>Whenever a Core Audio function provides a return value, capture that value and check for success or failure. On failure, make use of Xcode’s debugging features as described in <em><a href="https://developer.apple.com/library/archive/documentation/DeveloperTools/Conceptual/XcodeDebugging/000-Introduction/Introduction.html#//apple_ref/doc/uid/TP40007057" target="_blank" rel="noopener noreferrer">Xcode Debugging Guide</a></em>. If using an Objective-C method in your app, such as for configuring your audio session, take advantage the <em>error</em> parameter in the same way.</p><p>Be aware of dependencies between function calls. For example, you can start an audio processing graph only after you successfully initialize it. Check the return value of <code>AUGraphInitialize</code>.</p><p>If the function returns successfully, you can start the graph. If it fails, determine what went wrong. Check that all of your audio unit function calls leading up to initialization returned successfully. For an example of how to do this, look at the <code>-configureAndInitializeAudioProcessingGraph</code> method in the sample code project <em>Audio Mixer (MixerHost)</em>.</p><p>Second, if graph initialization is failing, take advantage of the <code>CAShow</code> function. This function prints out the state of the graph to the Xcode console. The sample code project <em>Audio Mixer (MixerHost)</em> demonstrates this technique as well.</p><p>Ensure that you are initializing each of your <code>AudioStreamBasicDescription</code> structures to 0, as follows:</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">AudioStreamBasicDescription stereoStreamFormat = {0};</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Initializing the fields of an ASBD to 0 ensures that no fields contain garbage data. (In the case of declaring a data structure in external storage—for example, as an instance variable in a class declaration—its fields are automatically initialized to 0 and you need not initialize them yourself.)</p><p>To print out the field values of an <code>AudioStreamBasicDescription</code> structure to the Xcode console, which can be very useful during development, use code like that shown in Listing 2-8.</p><p><strong>Listing 2-8</strong> A utility method to print field values for an <code>AudioStreamBasicDescription</code> structure</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-c codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token keyword" style="font-style:italic">void</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> printASBD</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">AudioStreamBasicDescription</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> asbd </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">char</span><span class="token plain"> formatIDString</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token number" style="color:rgb(247, 140, 108)">5</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    UInt32 formatID </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> CFSwapInt32HostToBig </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFormatID</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    bcopy </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token operator" style="color:rgb(137, 221, 255)">&amp;</span><span class="token plain">formatID</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> formatIDString</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">4</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    formatIDString</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token number" style="color:rgb(247, 140, 108)">4</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;\0&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Sample Rate:         %10.0f&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mSampleRate</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Format ID:           %10s&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    formatIDString</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Format Flags:        %10X&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFormatFlags</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Bytes per Packet:    %10d&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBytesPerPacket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Frames per Packet:   %10d&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mFramesPerPacket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Bytes per Frame:     %10d&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBytesPerFrame</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Channels per Frame:  %10d&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mChannelsPerFrame</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    NSLog </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">@</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;  Bits per Channel:    %10d&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">    asbd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">mBitsPerChannel</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>This utility method can quickly reveal problems in an ASBD.</p><p>When defining an ASBD for an audio unit stream format, take care to ensure you are following the &quot;Recommended stream format attributes” and “Stream format notes” in the usage tables in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1" target="_blank" rel="noopener noreferrer">Using Specific Audio Units</a>. Do not deviate from those recommendations unless you have a specific reason to.</p><blockquote><p>in external storage 外部存储</p></blockquote><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="using-specific-audio-units"></a>Using Specific Audio Units<a class="hash-link" href="#using-specific-audio-units" title="Direct link to heading">#</a></h2><p>Each iOS audio unit has certain things in common with all others and certain things unique to itself. Earlier chapters in this document described the common aspects, among them the need to find the audio unit at runtime, instantiate it, and ensure that its stream formats are set appropriately. This chapter explains the differences among the audio units and provides specifics on how to use them.</p><blockquote><p>among them 其间</p></blockquote><p>Later in the chapter, <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW14" target="_blank" rel="noopener noreferrer">Identifier Keys for Audio Units</a> lists the codes you need to locate the dynamically-linkable libraries for each audio unit at runtime.</p><blockquote><p>Later in the chapter 本章的后面</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="using-io-units"></a>Using I/O Units<a class="hash-link" href="#using-io-units" title="Direct link to heading">#</a></h3><p>iOS provides three I/O (input/output) units. The vast majority of audio-unit applications use the Remote I/O unit, which connects to input and output audio hardware and provides low-latency access to individual incoming and outgoing audio sample values. For VoIP apps, the Voice-Processing I/O unit extends the Remote I/O unit by adding acoustic echo cancelation and other features. To send audio back to your application rather than to output audio hardware, use the Generic Output unit.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="remote-io-unit"></a>Remote I/O Unit<a class="hash-link" href="#remote-io-unit" title="Direct link to heading">#</a></h4><p>The Remote I/O unit (subtype <code>kAudioUnitSubType_RemoteIO</code>) connects to device hardware for input, output, or simultaneous input and output. Use it for playback, recording, or low-latency simultaneous input and output where echo cancelation is not needed.</p><p>The device’s audio hardware imposes its audio stream formats on the outward-facing sides of the Remote I/O unit, as described in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW34" target="_blank" rel="noopener noreferrer">Understanding Where and How to Set Stream Formats</a>. The audio unit provides format conversion between the hardware audio formats and your application audio format, doing so by way of an included Format Converter audio unit.</p><p>For sample code that shows how to use this audio unit, see the sample code project <em><a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770" target="_blank" rel="noopener noreferrer">aurioTouch</a></em>.</p><p>Table 3-1 provides usage details for this audio unit.</p><p>Table 3-1  Using the Remote I/O unit</p><table><thead><tr><th align="left">Audio unit feature</th><th align="left">Details</th></tr></thead><tbody><tr><td align="left">Elements</td><td align="left">One input element: element 1. One output element: element 0.By default, the input element is disabled and the output element is enabled. If you need to change this, refer to the description of the <code>kAudioOutputUnitProperty_EnableIO</code> property.</td></tr><tr><td align="left">Recommended stream format attributes</td><td align="left"><code>kAudioFormatLinearPCM``AudioUnitSampleType``kAudioFormatFlagsAudioUnitCanonical</code></td></tr><tr><td align="left">Stream format notes</td><td align="left">The outward-facing sides of the Remote I/O unit acquire their formats from the audio hardware as follows:The input element (element 1) input scope gets its stream format from the currently-active audio input hardware.The output element (element 0) output scope gets its stream format from the currently-active output audio hardware.Set your application format on the output scope of the input element. The input element performs format conversion between its input and output scopes as needed. Use the hardware sample rate for your application stream format.If the input scope of the output element is fed by an audio unit connection, it acquires its stream format from that connection. If, however, it is fed by a render callback function, set your application format on it.</td></tr><tr><td align="left">Parameters</td><td align="left">None in iOS.</td></tr><tr><td align="left">Properties</td><td align="left">See <code>I/O Audio Unit Properties</code>.</td></tr><tr><td align="left">Property notes</td><td align="left">You never need to set the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property on this audio unit.</td></tr></tbody></table><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="voice-processing-io-unit"></a>Voice-Processing I/O Unit<a class="hash-link" href="#voice-processing-io-unit" title="Direct link to heading">#</a></h4><p>The Voice-Processing I/O unit (subtype <code>kAudioUnitSubType_VoiceProcessingIO</code>) has the characteristics of the Remote I/O unit and adds echo suppression for two-way duplex communication. It also adds automatic gain correction, adjustment of voice-processing quality, and muting. This is the correct I/O unit to use for VoIP (Voice over Internet Protocol) apps.</p><p>All of the considerations listed in <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW10" target="_blank" rel="noopener noreferrer">Table 3-1</a> apply as well to the Voice-Processing I/O unit. In addition, there are specific properties available for this audio unit, described in <code>Voice-Processing I/O Audio Unit Properties</code>.</p><blockquote><p>adds echo suppression for two-way duplex communication.  为双向双向通信增加了回声抑制</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="generic-output-unit"></a>Generic Output Unit<a class="hash-link" href="#generic-output-unit" title="Direct link to heading">#</a></h4><p>Use this audio unit, of subtype <code>kAudioUnitSubType_GenericOutput</code>, when sending the output of an audio processing graph to your application rather than to the output audio hardware. You would typically use the Generic Output unit for offline audio processing. Just like the other I/O units, the Generic Output unit incorporates a Format Converter unit. This lets the Generic Output unit perform format conversion between the stream format used in an audio processing graph and the format you want.</p><p>You can also use a Generic Output unit as the final node in a subgraph that you place into a parent audio processing graph.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="using-mixer-units"></a>Using Mixer Units<a class="hash-link" href="#using-mixer-units" title="Direct link to heading">#</a></h3><p>iOS provides two mixer units. In most cases, you should use the Multichannel Mixer unit, which provides mixing for any number of mono or stereo streams. If you need the features of the 3D Mixer unit, you should very likely be using OpenAL instead. OpenAL is built on top of the 3D Mixer unit, providing equivalent performance with a simpler API that is well suited for game app development.</p><blockquote><p>you should very likely be using OpenAL instead. 你可能更应该使用openAL</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="multichannel-mixer-unit"></a>Multichannel Mixer Unit<a class="hash-link" href="#multichannel-mixer-unit" title="Direct link to heading">#</a></h4><p>The Multichannel Mixer unit (subtype <code>kAudioUnitSubType_MultiChannelMixer</code>) takes any number of mono or stereo streams and combines them into a single stereo output. It controls audio gain for each input and for the output, and lets you turn each input on or off separately. Starting in iOS 4.0, the Multichannel Mixer supports stereo panning for each input.</p><p>For sample code that shows how to use this audio unit, see the sample code project <em>Audio Mixer (MixerHost)</em>.</p><p>Table 3-2 provides usage details for this audio unit.</p><p>Table 3-2 provides usage details for this audio unit.</p><table><thead><tr><th align="left">Audio unit feature</th><th align="left">Details</th></tr></thead><tbody><tr><td align="left">Elements</td><td align="left">One or more input elements, each of which can be mono or stereo. One stereo output element.</td></tr><tr><td align="left">Recommended stream format attributes</td><td align="left"><code>kAudioFormatLinearPCM``AudioUnitSampleType``kAudioFormatFlagsAudioUnitCanonical</code></td></tr><tr><td align="left">Stream format notes</td><td align="left">On the input scope, manage stream formats as follows:If an input bus is fed by an audio unit connection, it acquires its stream format from that connection.If an input bus is fed by a render callback function, set your complete application stream format on the bus. Use the same stream format as used for the data provided by the callback.On the output scope, set just the application sample rate.</td></tr><tr><td align="left">Parameters</td><td align="left">See <code>Multichannel Mixer Unit Parameters</code>.</td></tr><tr><td align="left">Properties</td><td align="left"><code>kAudioUnitProperty_MeteringMode</code>.</td></tr><tr><td align="left">Property notes</td><td align="left">By default, the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property is set to a value of 1024, which is not sufficient when the screen locks and the display sleeps. If your app plays audio with the screen locked, you must increase the value of this property unless audio input is active. Do as follows:If audio input is active, you do not need to set a value for the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property.If audio input is not active, set this property to a value of 4096.</td></tr></tbody></table><blockquote><p>which is not sufficient 这是不够的</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="3d-mixer-unit"></a>3D Mixer Unit<a class="hash-link" href="#3d-mixer-unit" title="Direct link to heading">#</a></h4><p>The 3D Mixer unit (subtype <code>kAudioUnitSubType_3DMixer</code>) controls stereo panning, playback tempo, and gain for each input, and controls other characteristics such as apparent distance to the listener. The output has an audio gain control. To get some idea of what this audio unit can do, consider that OpenAL in iOS is implemented using it.</p><p>In most cases, if you need the features of the 3D Mixer unit, your best option is to use OpenAL. For sample code that shows how to use OpenAL, see the sample code project <em>oalTouch</em>.</p><p>Table 3-3 provides usage details for this audio unit.</p><table><thead><tr><th align="left">Audio unit feature</th><th align="left">Details</th></tr></thead><tbody><tr><td align="left">Elements</td><td align="left">One or more input elements, each of which is mono. One stereo output element.</td></tr><tr><td align="left">Recommended stream format attributes</td><td align="left"><code>UInt16``kAudioFormatFlagsCanonical</code></td></tr><tr><td align="left">Stream format notes</td><td align="left">On the input scope, manage stream formats as follows:If an input bus is fed by an audio unit connection, it acquires its stream format from that connection.If an input bus is fed by a render callback function, set your complete application stream format on the bus. Use the same stream format as used for the data provided by the callback.On the output scope, set just the application sample rate.</td></tr><tr><td align="left">Parameters</td><td align="left">See <code>3D Mixer Unit Parameters</code>.</td></tr><tr><td align="left">Properties</td><td align="left">See <code>3D Mixer Audio Unit Properties</code>. Note, however, that most of these properties are implemented only in the Mac OS X version of this audio unit.</td></tr><tr><td align="left">Property notes</td><td align="left">By default, the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property is set to a value of 1024, which is not sufficient when the screen locks and the display sleeps. If your app plays audio with the screen locked, you must increase the value of this property unless audio input is active. Do as follows:If audio input is active, you do not need to set a value for the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property.If audio input is not active, set this property to a value of 4096.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="using-effect-units"></a>Using Effect Units<a class="hash-link" href="#using-effect-units" title="Direct link to heading">#</a></h3><p>The iPod EQ unit (subtype <code>kAudioUnitSubType_AUiPodEQ</code>) is the only effect unit provided in iOS 4. This is the same equalizer used by the built-in iPod app. To view the iPod app’s user interface for this audio unit, go to Settings &gt; iPod &gt; EQ. This audio unit offers a set of preset equalization curves such as Bass Booster, Pop, and Spoken Word.</p><p>You must supply your own user interface to the iPod EQ unit, as you must for any of the audio units. The <em><a href="https://developer.apple.com/library/archive/samplecode/iPhoneMixerEQGraphTest/Introduction/Intro.html#//apple_ref/doc/uid/DTS40009555" target="_blank" rel="noopener noreferrer">Mixer iPodEQ AUGraph Test</a></em> sample code project demonstrates how to use the iPod EQ unit and shows one way to provide a user interface for it.</p><p>Table 3-4 provides usage details for this audio unit.</p><table><thead><tr><th align="left">Audio unit feature</th><th align="left">Details</th></tr></thead><tbody><tr><td align="left">Elements</td><td align="left">One mono or stereo input element. One mono or stereo output element.</td></tr><tr><td align="left">Recommended stream format attributes</td><td align="left"><code>kAudioFormatLinearPCM``AudioUnitSampleType``kAudioFormatFlagsAudioUnitCanonical</code></td></tr><tr><td align="left">Stream format notes</td><td align="left">On the input scope, manage stream formats as follows:If the input is fed by an audio unit connection, it acquires its stream format from that connection.If the input is fed by a render callback function, set your complete application stream format on the bus. Use the same stream format as used for the data provided by the callback.On the output scope, set the same full stream format that you used for the input.</td></tr><tr><td align="left">Parameters</td><td align="left">None.</td></tr><tr><td align="left">Properties</td><td align="left"><code>kAudioUnitProperty_FactoryPresets</code> and <code>kAudioUnitProperty_PresentPreset</code></td></tr><tr><td align="left">Property notes</td><td align="left">The iPod EQ unit provides a set of predefined tonal equalization curves as factory presets. Obtain the array of available EQ settings by accessing the audio unit’s <code>kAudioUnitProperty_FactoryPresets</code> property. You can then apply a setting by using it as the value for the <code>kAudioUnitProperty_PresentPreset</code> property.By default, the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property is set to a value of 1024, which is not sufficient when the screen locks and the display sleeps. If your app plays audio with the screen locked, you must increase the value of this property unless audio input is active. Do as follows:If audio input is active, you do not need to set a value for the <code>kAudioUnitProperty_MaximumFramesPerSlice</code> property.If audio input is not active, set this property to a value of 4096.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="identifier-keys-for-audio-units"></a>Identifier Keys for Audio Units<a class="hash-link" href="#identifier-keys-for-audio-units" title="Direct link to heading">#</a></h3><p>This table provides the identifier keys you need to access the dynamically-linkable libraries for each iOS audio unit, along with brief descriptions of the audio units.</p><blockquote><p>along with brief descriptions of the audio units 以及简要的说明</p></blockquote><p><img alt="image-20210123223500263" src="/assets/images/image-20210123223500263-64684ba53e13d8ff2880a8e59f80b594.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="reference"></a>reference<a class="hash-link" href="#reference" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="document"></a>document<a class="hash-link" href="#document" title="Direct link to heading">#</a></h3><ul><li><p>AppleDocument: <a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html" target="_blank" rel="noopener noreferrer">https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html</a></p></li><li><p>For an overview of iOS audio APIs, and guidance on when to use each one, refer to <em><a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/MultimediaPG/Introduction/Introduction.html#//apple_ref/doc/uid/TP40009767" target="_blank" rel="noopener noreferrer">Multimedia Programming Guide</a></em>.</p></li><li><p>To work with audio units directly—configuring and controlling them—use the functions described in <em><a href="https://developer.apple.com/documentation/audiounit/audio_unit_component_services" target="_blank" rel="noopener noreferrer">Audio Unit Component Services Reference</a></em>.</p></li><li><p>To create and configure an audio processing graph (a processing chain of audio units) use the functions described in <em><a href="https://developer.apple.com/documentation/audiotoolbox/audio_unit_processing_graph_services" target="_blank" rel="noopener noreferrer">Audio Unit Processing Graph Services Reference</a></em>.</p></li><li><p>Core Audio Glossary：<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004453-CH211-SW1" target="_blank" rel="noopener noreferrer">https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004453-CH211-SW1</a> （音频术语）</p></li><li></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="sample-code"></a>sample code<a class="hash-link" href="#sample-code" title="Direct link to heading">#</a></h3><ul><li>For sample code that shows how to use the Remote I/O unit, see the sample code project <em><a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770" target="_blank" rel="noopener noreferrer">aurioTouch</a></em>.</li></ul></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="dys-typora-open://mine/survival/docs/音视频/学习资料/教程/Audio-Unit-Hosting-Guide-for-iOS.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#audio-unit-hosting-fundamentals" class="table-of-contents__link">Audio Unit Hosting Fundamentals</a><ul><li><a href="#audio-units-provide-fast-modular-audio-processing" class="table-of-contents__link">Audio Units Provide Fast, Modular Audio Processing</a></li><li><a href="#audio-processing-graphs-manage-audio-units" class="table-of-contents__link">Audio Processing Graphs Manage Audio Units</a></li><li><a href="#render-callback-functions-feed-audio-to-audio-units" class="table-of-contents__link">Render Callback Functions Feed Audio to Audio Units</a></li><li><a href="#audio-stream-formats-enable-data-flow" class="table-of-contents__link">Audio Stream Formats Enable Data Flow</a></li></ul></li><li><a href="#constructing-audio-unit-apps" class="table-of-contents__link">Constructing Audio Unit Apps</a><ul><li><a href="#start-by-choosing-a-design-pattern" class="table-of-contents__link">Start by Choosing a Design Pattern</a></li><li><a href="#constructing-your-app" class="table-of-contents__link">Constructing Your App</a></li><li><a href="#troubleshooting-tips" class="table-of-contents__link">Troubleshooting Tips</a></li></ul></li><li><a href="#using-specific-audio-units" class="table-of-contents__link">Using Specific Audio Units</a><ul><li><a href="#using-io-units" class="table-of-contents__link">Using I/O Units</a></li><li><a href="#using-mixer-units" class="table-of-contents__link">Using Mixer Units</a></li><li><a href="#using-effect-units" class="table-of-contents__link">Using Effect Units</a></li><li><a href="#identifier-keys-for-audio-units" class="table-of-contents__link">Identifier Keys for Audio Units</a></li></ul></li><li><a href="#reference" class="table-of-contents__link">reference</a><ul><li><a href="#document" class="table-of-contents__link">document</a></li><li><a href="#sample-code" class="table-of-contents__link">sample code</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">Style Guide</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/">Second Doc</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord</a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/styles.9f2bd08d.js"></script>
<script src="/runtime~main.b9916a1e.js"></script>
<script src="/main.aad198f0.js"></script>
<script src="/1.c6142e7c.js"></script>
<script src="/259.bd219e04.js"></script>
<script src="/260.8c75c7b8.js"></script>
<script src="/935f2afb.b83995d3.js"></script>
<script src="/258.77a3145c.js"></script>
<script src="/f30035aa.b5f636a0.js"></script>
</body>
</html>